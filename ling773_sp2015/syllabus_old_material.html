

<!--

<tr>
<td> </td>
<td>Lexical acquisition<BR>
</td>
<td>Ch 8 (exc 8.5)</td>
<td>
  <!- -
    <A
    HREF="http://umiacs.umd.edu/~resnik/ling773_sp2015/assignment/lexical_acquisition.html">Assignment
    </td>
  - ->
<td></td>
</tr>

<!- -
<tr>
<td> </td>
<td>Compositional semantics (?)
</td>
<td>
<A
HREF="http://www.stanford.edu/class/cs224n/handouts/cl-semantics-new.pdf">Manning
(2000; rev 2005), An Introduction to Formal Computational Semantics</A>
</td>
<td></td>
<td></td>
</tr>

<tr>
<td> </td>
<td>Computational psycholinguistics (?)<BR>
</td>
<td>
<A HREF="http://www.cs.colorado.edu/~martin/slp.html">
Jurafsky and Martin (2000)</A>, Sections 12.5 (Human Parsing) and 13.4
(Complexity and Human Processing);
<A HREF="http://www.stanford.edu/~jurafsky/prob.pdf">Jurafsky
(2003)</A> (except Sections 2.1, 2.2, 2.5, 2.6, 3.1, 3.5, 3.6).
Connectionist modeling?

(From published book: except Sections 3.2.1, 3.2.2, 3.2.5, 3.2.6, 3.3.1, 3.3.5, 3.3.6)



<tr>
<td> </td>
<td>TBA<BR>
</td>
<td></td>
<td></td>
<td></td>
</tr>

- ->

<!- -
<tr>
<td><font color="red">FINAL DATE</font></td>
<td><font color="red">TBD</font><BR>
</td>
<td>Officially cumulative but with a strong (at least 80%) emphasis
on material after the midterm.
</td>
<td>Relax!</td>
<td><A HREF="final_guidance.html">Guidance for studying</A> </td>
</tr>
- ->



<!- -
Jurafsky, Dan. 2003. Probabilistic Modeling in Psycholinguistics:
Linguistic Comprehension and Production. In Rens Bod, Jennifer
Hay, and Stefanie Jannedy, (Eds)., Probabilistic Linguistics.
http://www.stanford.edu/~jurafsky/prob.pdf

Rick Lewis, Computational Psycholinguistics, Encylopedia of Cognitive
Science, Macmillan, 2000.
www-personal.umich.edu/~rickl/Documents/Lewis-CompPsychling.pdf

Crocker, Matthew W. and Frank Keller. 2005. Probabilistic Grammars as
Models of Gradience in Language Processing. To appear in Gisbert
Fanselow, Caroline F	ry, Ralph Vogel, and Matthias Schlesewsky,
eds., Gradience in Grammar: Generative Perspectives. Oxford: Oxford
University Press.
http://homepages.inf.ed.ac.uk/keller/papers/oup05b.pdf

Lewis & Vasishth on interference
http://www.msu.edu/course/lin/875/cogsci-04-jou.pdf

Levy on surprisal and German
http://www.msu.edu/course/lin/875/surprisal-chapter.pdf

Hale syllabus
http://www.msu.edu/course/lin/875/syllabus.html

Morten H. Christiansen,
Connectionist psycholinguistics: The very idea.
In M.H. Christiansen & N. Chater (Eds.), Connectionist
psycholinguistics (pp.1-15). Westport, CT: Ablex.
http://cnl.psych.cornell.edu/papers/CP-intro.pdf

John Nerbonne learning bibliography
http://www.let.rug.nl/~nerbonne/teach/learning/literature.htm
- ->


</table>
</center>
</font>
<P>




<!- -
<A NAME="labs">
*We may or may not add a few lab sessions, but if we do they will be
held in room 1442 of the <A
HREF="http://www.inform.umd.edu/CampusInfo/Facilities/Buildings/AVW/">
     A.V. Williams building</A>.  Click the link for maps, directions, parking
     information.  To get to Room 1442 come in the main entrance, facing
     the elevators, turn left, and go through the glass doors.  The lab
     will be on your right.
- ->
<P>



<A HREF="index.html">Return to course home page</A>
<HR>

<hr>
<P>
</body>
</html>




<!-- ################ OLD STUFF ################# -->
<!--
<tr>
<td>March 27</td>
<td>More on supervised learning: maximum entropy models and conditional random fields <font color="red">[Guest lecturer TBA]</font><BR>
</td>
<td>
  <A HREF="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.63.2111">
   Using maximum entropy for text classification (Kamal Nigam, John Lafferty, Andrew McCallum)</A>;
  <A HREF="http://www.aclweb.org/anthology/N/N03/N03-1028.pdf">
   Shallow Parsing with Conditional Random Fields (Fei Sha and Fernando Pereira)</A>
<BR>
 <em>The maximum entropy principle;
     maxent classifiers (for predicting a single variable);
      CRFs (for predicting interacting variables);
      L2 regularization.
  </td>
<td></td>
</td>
<td>
   Optionally, some good introductory material appears in
   <A  HREF="http://www-2.cs.cmu.edu/afs/cs/user/aberger/www/html/tutorial/tutorial.html">
   Adam Berger's maxent tutorial</A>,
   Dan Klein and Chris Manning's
   <A HREF="http://www.cs.berkeley.edu/~klein/papers/maxent-tutorial-slides.pdf">
   Maxent Models, Conditional Estimation, and Optimization, without the Magic</A>, and
   Noah Smith's  <A HREF="http://umiacs.umd.edu/~resnik/ling848_fa2004/slides/loglinear_handout.pdf">
   notes on loglinear models</A> (which provides explicit details for a lot of the math).
   Another useful reading, focused on estimating the parameters of maxent models, is
   <A HREF="http://bulba.sdsu.edu/~malouf/papers/conll02.pdf">A comparison of algorithms for maximum
   entropy parameter estimation (Rob Malouf)</A>.
   Also, Manning and Schuetze section 16.2 can be read as supplementary material.
   Of historical interest:
   Adwait Ratnaparkhi's <A
   HREF="ftp://ftp.cis.upenn.edu/pub/ircs/tr/97-08.ps.Z">A Simple
   Introduction to Maximum Entropy Models for Natural Language
   Processing</A> (1997).
</td>
</tr>
-->



<!--
<tr>
<td>Apr 22
<td>Information retrieval; guest lecture (Smaranda Muresan) on graph-based methods in NLP<BR>
</td>
<td>
(a) Rada Mihalcea and Paul Tarau, <A
HREF="http://www.cs.unt.edu/~rada/papers/mihalcea.emnlp04.pdf">TextRank:
Bringing Order into Texts</A>, in Proceedings of the Conference on
Empirical Methods in Natural Language Processing (EMNLP 2004),
Barcelona, Spain, July 2004.;
(b) Rada Mihalcea, <A
HREF="http://www.cs.unt.edu/~rada/papers/mihalcea.acl2004.pdf">Graph-based
Ranking Algorithms for Sentence Extraction, Applied to Text
Summarization</A>, in Proceedings of the 42nd Annual Meeting of the
Association for Computational Linguistics, companion volume (ACL
2004), Barcelona, Spain, July 2004;
(c) Paper/data of Pang and Lee on <A
HREF="http://www.cs.cornell.edu/home/llee/papers/cutsent.home.html">sentiment
analysis with min-cuts</A>
<P>
<em>PageRank and variants; HITS; min-cuts</em>
</td>
<td> <A HREF="slides/jimmy_ir_lecture.ppt">IR lecture slides</A>,<BR>
     <A HREF="slides/graph_based_methods.ppt">Graph-methods lecture
     slides</A>.
</td>
<td>
Optional readings of interest:
(a) Christopher D. Manning, Prabhakar Raghavan and Hinrich Schutze,
<A HREF="http://www-csli.stanford.edu/~schuetze/information-retrieval-book.html">Introduction to Information Retrieval, Cambridge University Press</A>:
<A HREF="http://nlp.stanford.edu/IR-book/pdf/chapter21-linkanalysis.pdf">
Chapter 21 "Link Analysis"</A>;
(b) <A HREF="http://dbpubs.stanford.edu:8090/pub/1999-66">Page L. et. al Page Rank Citation Ranking: Bringing Order to the Web</A>;
(c) <A HREF="http://www.cs.cornell.edu/home/kleinber/auth.pdf">Jon Kleinberg  Authoritative sources in a hyperlinked environment, in proceedings of SODA 1998</A>
(d) Kurt Bryan and Tanya Leise, <A  HREF="http://www.rose-hulman.edu/~bryan/google.html">The $25,000,000,000 Eigenvector: The Linear Algebra Behind Google</A> (SIAM Review 48(3), 2006, pp. 569-581)
<td>
-->

<!--
<tr>
<td>Apr 17</td>
<td>Word sense disambiguation<BR>
</td>
<td>Ch 8.5, 15.{1,2,4}<BR>
<em>
 Semantic similarity; relatedness; synonymy; polysemy; homonymy; entailment; ontology-based similarity measures; vector
 representations and similarity measures; sketch of LSA.
 Characterizing the WSD problem; WSD as a  supervised classification problem. Lesk algorithm;
 semi-supervised learning and Yarowsky's algorithm;
 WSD in applications; WSD evaluation.
</em>
</td>
<td></td>
<td>
Optional:  Adam Kilgarriff (1997) <A HREF="http://www.kilgarriff.co.uk/Publications/1997-K-CHum-believe.pdf">I don't believe in word senses</A> Computers and the Humanities 31(2), pp. 91-113;
Philip Resnik (2006), <A HREF="http://www.springerlink.com/content/j227415g22v74686/">WSD in NLP Applications</A> (<A HREF="http://books.google.com/books?id=GLck75U20pAC&lpg=PA299&ots=M3uAfkLHxb&dq=resnik%20wsd%20in%20nlp%20applications&pg=PA299#v=onepage&q=resnik%20wsd%20in%20nlp%20applications&f=false">Google Books</A>)
  </td>
</tr>
-->



<!-- 
<tr>
<td>May 6 </td>
<td>Machine translation continued<br></td>
<td>
  <!- - <font color="red">This material may be folded into the previous class
  in order to make room for a different topic.</font><p>
  Papineni, Roukos, Ward and Zhu. 2001.
  <A HREF="http://www1.cs.columbia.edu/nlp/sgd/bleu.pdf">BLEU: A Method for Automatic Evaluation
  of Machine Translation</A>
  - ->
  <P>
  <em><!- -COVER Components of a phrase-based system: language modeling,
  translation modeling; sentence alignment, word
  alignment, phrase extraction, parameter tuning, decoding, rescoring,
  evaluation. - -></em>
  </td>
<td></td>
<!- -<td> <A HREF="assignments/phrase_based_mt.html">Assignment 6</A><
  or continue Team Project 2   </td> - ->
<td>
<!- - Hal DaumÃ© III and Jagadeesh Jagarlamudi,
 <A HREF="http://hal3.name/docs/daume11lexicaladapt.pdf">
 Domain Adaptation for Machine Translation by Mining Unseen Words</A>, ACL 2011.
- ->
</td>
</tr>
-->


<!--
<tr>
<td>Apr 30
<td>The Web as a Corpus<BR>
</td>
<td>
  (a) A. Kilgarriff and G. Grefenstette, <A
  HREF="http://citeseer.ist.psu.edu/630648.html">Introduction to the
  special issue on the web as corpus</A>, Computational Linguistics
  29(3): 333-348 (2003) <BR>
  (b) Lapata, Mirella and Frank Keller. 2004. <A
  HREF="http://homepages.inf.ed.ac.uk/mlap/Papers/naacl04a.html">The
  Web as a Baseline: Evaluating the Performance of Unsupervised
  Web-based Models for a Range of NLP Tasks</A>. Proc HLT/NAACL,
  pp. 121-128.
  <BR>
  (c) Lapata, Maria. 2001. <A
  HREF="http://homepages.inf.ed.ac.uk/mlap/Papers/naacl01.html">A
  Corpus-based Account of Regular Polysemy: The Case of
  Context-sensitive Adjectives.</A>, Proc NAACL.
  <BR>
  (d) Philip Resnik, Aaron Elkiss, Ellen Lau and Heather Taylor.  <A
  HREF="http://umiacs.umd.edu/~resnik/pubs/bls2005.pdf">
  The Web in Theoretical Linguistics Research: Two Case Studies Using
  the Linguist's Search Engine.</A>, Proc. 31st Meeting of the Berkeley
  Linguistics Society, pp. 265-276, February 2005.
  <P>
  <em>
  What is a corpus?; using the Web for NLP tasks; ways linguists can use the Web.
  </em>
</td>
<td></td>
<td>
Also of possible interest:
  <A HREF="http://lse.umiacs.umd.edu">Linguist's Search Engine</A>;
  <BR>
  Mirella Lapata, and Frank Keller. 2005. <A
  HREF="http://homepages.inf.ed.ac.uk/mlap/Papers/tslp05.html">Web-based
  Models for Natural Language Processing.<A> ACM Transactions on
  Speech and Language Processing 2:1, 1-31. (Extends Lapata and Keller 2004);
  <BR>
  <A HREF="http://www.webexp.info/">WebExp</A> software for
  Web-based psycholinguistics
</td>
<td>
-->

<!-- <A HREF="http://mitpress.mit.edu/journals/pdf/coli_29_3_333_0.pdf">
Kilgarriff and Grefenstette (2003)</A>;
<A HREF="http://homepages.inf.ed.ac.uk/keller/papers/cl03.pdf">Keller
and Lapata (2003)</A>
  Keller, Frank and Mirella Lapata, <A
HREF="http://homepages.inf.ed.ac.uk/keller/papers/cl03.pdf">Using the
Web to Obtain Frequencies for Unseen Bigrams. Computational
Linguistics 29:3, 459-484, 2003;
Philip Resnik and Aaron Elkiss, <A
HREF="http://umiacs.umd.edu/~resnik/temp/lsedemo_for_class.pdf"> The
Linguist's Search Engine: An Overview [DRAFT]</A>; others?
</td>
</tr>
-->

