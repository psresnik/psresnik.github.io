<html>
<BASE  HREF="http://umiacs.umd.edu/~resnik/ling773_sp2016/index.html">
<title>Schedule of Topics</title>
<body bgcolor="#ffffff">
<FONT FACE="Arial, Helvetica, Geneva" SIZE=-1>
<hr>
<h1>Schedule of Topics</h1>
<hr>
<P>

<!-- <A HREF="midterm_guidance.html">Guidance for midterm studying</A> -->

This is the schedule of topics for
<A HREF="http://umiacs.umd.edu/~resnik/ling773_sp2016/index.html">
Computational Linguistics II, Spring 2016</A>.
<P>
In readings, "M&S" refers to
Christopher D. Manning and Hinrich Schuetze,
<A HREF="http://cognet.mit.edu/library/books/view?isbn=0262133601">Foundations
of Statistical Natural Language Processing</A>.
The "other" column has optional links pointing either
to material you should already know (but might want to review), or to
related material you might be interested in.
Make sure to do your reading <strong>before</strong> the class where it is listed!<P>
<P>

<font color="red" size=>THIS SCHEDULE IS A WORK IN PROGRESS!  <BR>
In addition, some topic areas may take longer than expected, so keep
an eye on the online class discussions for "official"
dates.</font>

<P>
See <A HREF="https://talks.cs.umd.edu/lists/7">CL Colloquium Talks</A> for possible extra credit each week.


<center>
<table  border=1 cellpadding=4 bgcolor=white>
<tr>
 <th>Class</th>
 <th>Topic<BR></th>
 <th>Readings<A HREF="#readings">*</A></th>
 <th>Assignments</th>
 <th>Other</th>
</tr>

<tr>
<td>Jan 27</td>
<td>Course administrivia, semester plan; some statistical NLP fundamentals<BR>
</td>
<td>M&S Ch 1, 2.1.[1-9] (for review)<BR>
<em>
  <!--COVER Historical overview; Zipf's law; Probability spaces; finite-state and Markov models; Bayes' Rule; Bayesian updating; conjugate priors-->
  <!-- Word counts; tokenization; frequency and Zipf's law; concordances -->
</em>
</td>
<td>

  <A HREF="http://umiacs.umd.edu/~resnik/ling773_sp2016/assignments/assignment1.html">Assignment 1</A>

  <P>
</td>
<td>
<A HREF="http://languagelog.ldc.upenn.edu/">Language Log</A> (<em>the</em> linguistics blog);
<A href="http://nlpers.blogspot.com/">Hal Daum&eacute;'s NLP blog</A> (excellent blog, often technical machine learning stuff, but just as often more general interest, make sure to read the comment threads also because they're often excellent)
  <!-- <A HREF="http://www.economist.com/science/displayStory.cfm?story_id=3576374">Corpus Colossal</A> (The Economist, 20 Jan 2005);
  <A HREF="http://umiacs.umd.edu/~resnik/temp/lsedemo_for_class.pdf">
  Resnik and Elkiss (DRAFT)</A>;  <A HREF="http://lse.umiacs.umd.edu/">Linguist's Search Engine</A> -->
</td>
</tr>

<tr>
<td>Feb 3</td>
<td>Lexical association measures and hypothesis testing<BR>
</td>
<td>M&S Ch 5<BR>
<em><!--COVER Collocations;  mutual information; hypothesis testing--></em>
</td>
<td>

  <A HREF="http://umiacs.umd.edu/~resnik/ling773_sp2016/assignments/assignment2.html">Assignment 2</A>

  <P>
<td> 
<A HREF="http://www.aclweb.org/anthology/J93-1003">Dunning (1993)</A> is a classic and valuable to read if you're trying to use mutual information or chi-squared and getting inflated values for low-frequency observations.
<A HREF="http://research.microsoft.com/pubs/68957/rare-events-final-rev.pdf">Moore (2004)</A> is a less widely cited but very valuable discussion about how to judge the significance of rare events.
<P>
A really important paper by Ionnidis about problems with statistical hypothesis testing is
<A HREF="http://www.plosmedicine.org/article/info:doi/10.1371/journal.pmed.0020124">Why Most Published Research Findings Are False</A>;
for a very readable discussion see <A HREF="http://www.economist.com/news/briefing/21588057-scientists-think-science-self-correcting-alarming-degree-it-not-trouble">Trouble at the Lab</A>, The Economist, Oct 19, 2013 and the really great <A HREF="http://www.economist.com/blogs/graphicdetail/2013/10/daily-chart-2">accompanying video</A>. (Show that one to your friends and family!) For an interesting response, see <A HREF="http://www.edwardtufte.com/files/Study2.pdf">Most Published Research Findings Are False--But a Little Replication Goes a Long Way"</A>.
<P>
<A HREF="http://www.kilgarriff.co.uk/Publications/2005-K-lineer.pdf">Kilgarriff (2005)</A> is a fun and contrarian read regarding the use of hypothesis testing methodology specifically in language research.
<P>
<em>Named entities</em> represent another form of lexical association.  Named entity recognition is introduced in <A HREF="http://www.cs.colorado.edu/~martin/csci5417/ie-chapter.pdf">Jurafsky and Martin, Ch 22</A> and <A HREF="http://www.nltk.org/book/ch07.html">Ch 7 of the NLTK book</A>.

  <!--<A HREF="http://www.linguistics.ucsb.edu/faculty/stgries/research/NHST_CLLT.pdf">Gries (2005)</A>;<BR>-->
  <!--  <A  HREF="http://bmj.bmjjournals.com/cgi/content/full/310/6973/170">Bland and Altman (1995)</A>;<BR> -->
</td>
</tr>


<tr>
<td>Feb 10</td>
<td>Information theory<BR>
</td>
<td>M&S Ch 2.2, M&S Ch 6<BR>
<P>
Piantadosi et al. (2011), <A HREF="http://www.pnas.org/content/108/9/3526.long">Word lengths are optimized for efficient communication</A>;
Jaeger (2010), <A HREF="http://dx.doi.org/10.1016/j.cogpsych.2010.02.002">Redundancy and reduction: Speakers manage syntactic information density</A>
<em><!--COVER Information theory essentials; entropy, relative entropy, mutual
    information; noisy channel model; cross entropy and perplexity-->

</em>
</td>
<td>

<A HREF="http://umiacs.umd.edu/~resnik/ling773_sp2016/assignments/assignment3.html">Assignment 3</A>

  <!--  <A
    HREF="http://umiacs.umd.edu/~resnik/ling773_sp2016/assignments/info_theory.html">Assignment 2
  -->
  </td>
</td>
<td>

<A HREF="http://www.amazon.com/Elements-Information-Theory-Telecommunications-Processing/dp/0471241954">
Cover and Thomas (1991)</A> is a great, highly readable introduction to information theory.  The first few chapters go into many concepts from this lecture with greater rigor but a lot of clarity.<P>
 Maurits et al. (2010), <A HREF="http://papers.nips.cc/paper/4085-why-are-some-word-orders-more-common-than-others-a-uniform-information-density-account">Why are some word orders more common than
others? A uniform information density account</A>.  See also the syllabus for a 2009 seminar taught by Dan Jurafsky and Michael Ramscar, <A HREF="http://www.stanford.edu/class/psych227/">Information-Theoretic Models of Language and Cognition</A>, which looks as if it was awesome.

<P>

</td>
</tr>

<tr>
<td>Feb 17</td>
<td>Expectation Maximization<BR>
</td>
<td>Skim M&S Ch 9-10, Chapter 6 of <A HREF="http://www.umiacs.umd.edu/~jimmylin/book.html">Lin and Dyer</A>. Read
 <A HREF="http://umiacs.umd.edu/~resnik/ling773_sp2011/readings/em_recipe.v2.only_hmm.pdf">my EM recipe discussion.</A><BR>
<em><!--COVER Maximum likelihood estimation overview; quick review of smoothing; EM overview;
    interpolated estimation; Katz backoff; HMM as a noisy channel model
    HMM review; deriving forward-backward algorithm as an instance of EM; Viterbi algorithm review.-->
</em>
</td>
  <!-- <td><A  HREF="http://umiacs.umd.edu/~resnik/ling773_sp2016/assignments/smoothing.html">Assignment 3 </td> -->

<td>

<A  HREF="http://umiacs.umd.edu/~resnik/ling773_sp2016/assignments/assignment4.html">Assignment 4</A>

  <P>
<td>
<!--
 <A HREF="http://research.microsoft.com/~joshuago/tr-10-98.pdf">An
 empirical study of smoothing techniques for language modeling</A> (Stanley
 Chen and Joshua Goodman, Technical report TR-10-98, Harvard University,
 August 1998); -->
  <BR>
</td>
</tr>

<tr>
<td>Feb 24</td>
<td>Neural network language models (Lecture by Yogarshi Vyas)
</td>
<td>
<A HREF="http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.133.9693">Bengio et al (2003) <A>;
<A HREF="http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf">Mikolov et al (2010)</A>;
<A HREF="http://arxiv.org/abs/1301.3781">Mikolov et al. (2013)</A>
</td>
<td>
Before next class, review Yogarshi's <A HREF="slides/neural_network_LM_slides.pdf">slides</A> and do the <A HREF="http://cs.biu.ac.il/~yogo/nnlp.pdf">reading</A>.
 </td>
<td>
 </td>
</tr>


<tr>
<td>March 2</td>
<td>More on neural network models, deep learning, embeddings<P>
</td>
<td>
  Yoav Goldberg,  <A HREF="http://cs.biu.ac.il/~yogo/nnlp.pdf">A Primer on Neural Network Models for Natural Language Processing</A>. 
  Read with an emphasis on Sections 1-4 (largely covered in lecture last week), and Section 5 (embeddings); also look over 10-11 (RNNs) and 12 (recursive NNs) and the Wikipedia page on <A HREF="https://en.wikipedia.org/wiki/Autoencoder">autoencoders</A>.
  </td>
<td>
Assignment 5, worth 50% of a typical homework assignment: do one of the mini-projects from <A HREF="http://umiacs.umd.edu/~resnik/ling773_sp2016/assignments/assignment4.html">Assignment 4</A>. (If you already did one of these for the extra credit, you <em>must</em> do the other one!)
<P>
Note that Assignment 5 is <em>not</em> an extra-credit assignment.
  <P>
  </td>
<td>
Recommended: Sections 1, 3 and 4 of Yoshua Bengio, <A HREF="https://wiki.eecs.yorku.ca/course_archive/2012-13/F/6328/_media/learning-deep-ai.pdf">Learning Deep Architectures for AI</A>. Also recommended: the nice overview of representation learning in sections 1-4 of Bengio et al. <A HREF="http://arxiv.org/pdf/1206.5538v2.pdf">Representation Learning: A Review and New Perspectives</A>. 
<P>
Other useful background reading for broader perspective:
Lillian Lee, <A HREF="http://www.cs.cornell.edu/home/llee/papers/cf.pdf">Measures of Distributional Similarity</A>,
Hinrich Schuetze, <A HREF="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.41.8856">Word Space</A>,
Mikolov et al., <A HREF="http://research.microsoft.com/pubs/189726/rvecs.pdf">Linguistic Regularities in Continuous Space Word Representations</A>.
  </td>
</tr>


<tr>
<td>March 9</td>
<td>Bayesian graphical modeling, Gibbs sampling </td>
<td>
Philip Resnik and Eric Hardisty, <A HREF="http://umiacs.umd.edu/~resnik/pubs/gibbs.pdf">Gibbs Sampling for the Uninitiated</A>.  <P>
M. Steyvers and T. Griffiths (2007), <A HREF="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.80.9625&rep=rep1&type=pdf">Latent Semantic Analysis: A Road to Meaning</A> <em>and/or</em> review the CL1 topic modeling lecture (<A HREF="http://www.umiacs.umd.edu/~jbg/teaching/CMSC_723_2013/lecture_10.pdf">notes</A>, <A HREF="http://youtu.be/4p9MSJy761Y">video</A>).
<BR>
<em>
  <!--COVER [tentative]
  Graphical model representations of generative models; MLE, MAP, and Bayesian inference; Markov Chain Monte Carlo (MCMC)and Gibbs Sampling;
  Latent Dirichlet Allocation (LDA)-->
</em>
</td>
<td>
  <P>
  </td>
<td>
<!--
For a very nice and brief summary of LDA, including a really clear explanation of the corresponding Gibbs sampler (with pseudocode!), see Section 5 of Gregor Heinrich, <A HREF="http://faculty.cs.byu.edu/~ringger/CS601R/papers/Heinrich-GibbsLDA.pdf">Parameter estimation for text analysis</A>.
<P>
I may touch on supervised topic models; Blei and McAuliffe, <A HREF="https://www.cs.princeton.edu/~blei/papers/BleiMcAuliffe2007.pdf">Supervised Topic Models</A> (though note that we will not be talking about variational EM).  Also relevant is Nguyen, Boyd-Graber, and Resnik, <A HREF="http://www.umiacs.umd.edu/~jbg/docs/2013_shlda.pdf">Lexical and Hierarchical Topic Regression</A>.
<P>
If you're interested in going back to the source for LDA, see Blei, Ng, and Jordan (2003), <A HREF="http://www.cs.princeton.edu/~blei/papers/BleiNgJordan2003.pdf">Latent Dirichlet Allocation</A>.  
-->
</td>
</tr>


<tr>
<td>Mar 16</td>
<td><font color="red">Spring Break</font> <BR>
</td>
<td></td>
<td>Have fun!</td>
<td></td>
</tr>



<tr>
<td>March 23</td>
<td>Parsing<P>
</td>
<td>
M&amp;S Ch 11 (esp. pp. 381-388) and Ch 12 (esp. pp. 408-423, 448-455)
<P>
  </td>
<td>
<A HREF="http://www.umiacs.umd.edu/~resnik/ling773_sp2016/assignments/parsing.html">Assignment 6</A>
  </td>
<td>
A really nice article introducing parsing as inference is Shieber et al., <A HREF="http://www.eecs.harvard.edu/shieber/Biblio/Papers/infer.pdf">Principles and Implementation of Deductive Parsing</A> (see Sections 1-3), with a significant advance by Joshua Goodman, <A HREF="http://dl.acm.org/citation.cfm?id=973230">Semiring Parsing</A>.
  </td>
</tr>



<tr>
<td>Mar 30</td>
<td>Hal Daum&eacute;, Guest lecture <BR>
</td>
<td>
<A HREF="http://jmlr.csail.mit.edu/proceedings/papers/v15/ross11a/ross11a.pdf">A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning</A>,
St&eacute;phane Ross, Geoff J. Gordon, and J. Andrew Bagnell, AIStats 2011.
  </td>
<td>
<font color="red">Take-home midterm handed out</A></font>
 </td>
<td></td>
</tr>




<tr>
<td>April 6</td>
<td>
Structured prediction
</td>
<td>
Resources:
Noah Smith, <A HREF="http://www.cs.cmu.edu/~nasmith/LSP/">Linguistic structure prediction</A>;
Joshua Goodman, <A HREF="http://dl.acm.org/citation.cfm?id=973230">Semiring Parsing</A>
 </td>
<td>
 <A HREF="project/schizophrenia-project.pdf">Project</A> handed out. Project plans due next Wednesday.
<P>
</td>
<td>
</td>
</tr>



<tr>
<td>April 13</td>
<td>Evaluation
</td>
<td>
<!--
Supervised classification: M&S Ch 16 <em>except</em> 16.2.1;
Hearst et al. 1998 <A HREF="http://www.svms.org/tutorials/Hearst-etal1998.pdf">Support Vector Machines</A> (cleaner copy  <A HREF="https://web.archive.org/web/20130729024316/http://1024.ii.uni.wroc.pl/~aba/teach/SSNiS/CKpaper2.pdf">here</A>)<P>
-->
<font color="red">No class: Philip home sick.</A></font> Please use the class time to meet about your projects,
and please read Lin and Resnik, <A HREF="https://web.archive.org/web/20130421055104/http://www.umiacs.umd.edu/~jbg/teaching/CMSC_773_2012/reading/evaluation.pdf">Evaluation of NLP Systems</A>, Ch 11 of Alex Clark, Chris Fox and Shalom Lappin, eds., Blackwell Computational Linguistics and Natural Language Processing Handbook.
 </td>
<td>
 </td>
<td>
<!--
Hearst et al. (1998) is a nice  SVM reading because it's the clearest, shortest possible introduction.  There are many other good things to read at <A HREF="http://www.svms.org/">svms.org</A>, including a "best tutorials" section, broken out by introductory, intermediate, and advanced, under <A HREF="http://www.svms.org/tutorials/">Tutorials</A>.   Feel free to go with one of the other tutorials (the ones I've seen used most often are <A HREF="">Burges 1998</A> and <A HREF="">Smola et al. (1999)</A>)) instead of Hearst if you want a meatier introduction.
-->
</td>
</tr>



<tr>
<td>April 20</td>
<td>More on structured prediction; Machine translation<BR>
</td>
<td>
Ke Wu, <a href="../ling773_sp2013/seq-label.pdf">Discriminative Sequence Labeling</a>; 
Koehn, <A HREF="http://www.statmt.org/book/">Statistical Machine Translation</A>;
Chiang, <A HREF="http://www.aclweb.org/anthology/P05-1033">A Hierarchical Phrase-Based Model for Statistical Machine Translation</A>
<!--
Noah Smith, <A HREF="http://www.cs.cmu.edu/~nasmith/LSP/">Linguistic Structure Prediction</A>, esp. Sections 3.5.2-3.7. (The book is available <A HREF="http://dx.doi.org/10.2200/S00361ED1V01Y201105HLT013">online</A> for UMD and many other university IP addresses.)  See also Noah Smith's <A HREF="http://www.cs.cmu.edu/~nasmith/slides/sp4nlp.icml09.pdf">Structured prediction for NLP</A> tutorial slides (ICML'09).
-->
</td>
<td>
<P>
</td>
<td>
Ratnaparkhi (1996), <A HREF="http://acl.ldc.upenn.edu/W/W96/W96-0213.pdf">A Maximum Entropy Model for Part of Speech Tagging</A>, or, if you want a little more detail, Ratnaparkhi (1997), <A HREF="http://repository.upenn.edu/cgi/viewcontent.cgi?article=1083&context=ircs_reports">A Simple Introduction to Maximum Entropy Models for Natural Language Processing</A>.  Ratnaparkhi 1996 began the popularization of maxent in NLP.  Noah Smith's (2004) <A HREF="http://www.cs.cmu.edu/~nasmith/papers/smith.tut04.pdf">Log-Linear Models</a> is a nice alternative introduction  expressed in a vocabulary that is more consistent with current work.
<!--
<P>
For relevant background on semirings see Joshua Goodman, <A HREF="http://acl.ldc.upenn.edu/J/J99/J99-4004.pdf">Semiring Parsing</A>. For an example of very interesting recent work in this area particularly from an automata-theoretic angle, see Chris Dyer, <A HREF="http://ling.umd.edu/assets/publications/chris.dyer.diss.pdf">A Formal Model of Ambiguity and its Applications in Machine Translation</A>.
-->
<P>
Also of interest:
<ul>
<li> Hanna Wallach's <A HREF="http://www.inference.phy.cam.ac.uk/hmw26/papers/crf_intro.pdf">"Conditional Random Fields: An Introduction"</A>. 
<li> Charles Sutton and Andrew McCallum's <a href="http://homepages.inf.ed.ac.uk/csutton/publications/crftut-fnt.pdf">CRF tutorial</a> in 
Foundations and Trends of Machine Learning.
</ul>
<P>
Montreal's <A HREF="http://104.131.78.120/">LISA laboratory</A> is an epicenter for work on neural machine translation.
<!--
<li> Ben Taskar et al.'s <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.101.6014&rep=rep1&type=pdf">dissertation</a> on large margin training of structured prediction models.
<li> <a href="http://luthuli.cs.uiuc.edu/~daf/courses/learning/StructureLearning/tsochantaridis05a.pdf">Another paper</a> by Ioannis Tsochantaridis et al. on the same topic with a different approach.
<li> A nice <a href="http://www.chokkan.org/software/crfsuite/benchmark.html">benchmark comparison</a> of several algorithms for training CRFs and structured perceptrons.
</ul>
-->
</td>
</tr>


<tr>
<td>Apr 27</td>
<td>Framing (with guest Sarah Oates)</td>
<td>

Entman (2003), <A HREF="entman2003.pdf">Cascading Activation: Contesting the White House's Frame After 9/11</A>;
Oates, <A HREF="oates_framing.docx">Framing and Agenda-Setting Theory: Widening the Linguistic Lens</A>;
Greene, Stephan and Philip Resnik, <A HREF="https://pdfs.semanticscholar.org/3a81/77dc1b3d7aa4948f6cab50a1d3545e476617.pdf">More than Words: Syntactic Packaging and Implicit Sentiment</A>, NAACL (2009); 
Nguyen et al., <A HREF="https://pdfs.semanticscholar.org/540c/bf215ec82f2ea53d99e8e15f1d7c88c2f9f8.pdf">Tea Party in the House: A Hierarchical Ideal Point Topic Model and Its Application to Republican Legislators in the 112th Congress</A>, ACL 2015.

<!--
Dagan et al. (2009), <A HREF="http://research.microsoft.com/pubs/192037/RTE%20Special%20issue%20organizers%20final%20from%20journal-download.pdf">Recognizing textual entailment: Rationale, evaluation and approaches</A>
-->
<!--
M&amp;S Ch 13 and Adam Lopez, <A HREF="http://homepages.inf.ed.ac.uk/alopez/papers/survey.pdf">Statistical Machine Translation</A>,
In  ACM Computing Surveys  40(3), Article 8, pages 149, August 2008.<BR>
-->
</td>
<!-- <A HREF="assignments/parallel_text.html">Assignment</A> or Team
     Project 2 -->
<td>
  </td>
<td>
<!--
  Also potentially useful or of interest:
  <UL>
  <LI> Kevin Knight, <A HREF="http://www.isi.edu/natural-language/mt/wkbk.rtf">A Statistical MT Tutorial Workbook</A>;
  <LI> Philipp Koehn, <A
  HREF="http://www.isi.edu/licensed-sw/pharaoh/manual-v1.2.ps">PHARAOH: A
  Beam Search Decoder for Phrase-Based Statistical Machine
  Translation</A>
  <LI> Philipp Koehn (2004) <A HREF="http://www.iccs.informatics.ed.ac.uk/~pkoehn/publications/pharaoh-amta2004-slides.pdf">presentation on PHARAOH decoder</A>
  </UL>
  <BR>
  <A HREF="http://www.cs.unt.edu/~rada/papers/mihalcea.wpt03.pdf">
  Mihalcea and Pedersen (2003)</A>;
  <BR>
  Philip Resnik, <A HREF="http://umiacs.umd.edu/~resnik/pubs/cicling2004.ps">
  Exploiting Hidden Meanings: Using Bilingual Text for
  Monolingual Annotation</A>. In Alexander Gelbukh (ed.),  Lecture Notes
  in Computer Science 2945: Computational Linguistics and Intelligent
  Text Processing, Springer, 2004, pp. 283-299.
  -->
I have been called for jury duty on April 27. Prof. Sarah Oates of the Journalism School has kindly agreed to do a guest lecture, which will be either longer (if jury duty keeps me from class) or shorter (if it doesn't). In the latter case, she'll speak from the social science perspective in the first part of class, and I will talk about computational approaches to these issues in the second part of class.
<P>
<font color="red">Either way, you will be required to write a short summary, which will count as part of your homework grade.</A> 
<P>

</td>
</tr>

<tr>
<td>May 4</td>
<td>More on machine translation <BR></td>
<td>
M&amp;S Ch 13 and Adam Lopez, <A HREF="http://homepages.inf.ed.ac.uk/alopez/papers/survey.pdf">Statistical Machine Translation</A>,
In  ACM Computing Surveys  40(3), Article 8, pages 149, August 2008.<BR>
</td>
<td>
<P>
</td>
<td></td>
</tr>
</table>
</center>


<!--
<tr>
<td>May 13</td>
<td>Make-up/Projects presentations? <BR></td>
<td>May 12 is the last day of classes.  However, I am reserving this date in case we wish to use it, e.g. for a make-up class if we have snow cancellations, or for final project presentations.   </td>
<td></td>
<td></td>
</tr>
-->

