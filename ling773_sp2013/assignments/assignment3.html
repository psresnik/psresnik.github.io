<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2//EN">
<HTML>
<HEAD>
   <TITLE>Information theory questions</TITLE>
</HEAD>
<BODY BGCOLOR="#FFFFFF">

<HR>
<H1>Information Theory Assignment</H1>
<HR>

<P>

<font color=red>
When turning in this assignment, please make sure you <em>carefully</em> follow the directions from the TA on how to do so.  Remember, we're taking off points if you don't follow directions.  Thanks.
</font>


<P>
First, a couple of notes.
<P>
For the mathematically inclined and/or those who liked the discussion about
deriving an information measure from a set of properties such a measure should have,
here are <A
HREF="http://www.mtm.ufsc.br/~taneja/book/node5.html">several derivations</A> 
of the definition of entropy based on axioms about
properties that a measure of uncertainty should have.  
The one I used in class is from A.I. Khinchin,
<em>Mathematical Foundations of Information Theory</em>, Dover, New
York, 1957.
<P>
Also, in case you're curious, here's the pointer to the algorithm for
creating optimal codes using <A
HREF="http://en.wikipedia.org/wiki/Huffman_coding">Huffman coding</A>
(see also this <A
HREF="http://web.archive.org/web/20110518080824/http://www.huffmancoding.com/my-family/my-uncle/huffman-algorithm">interesting
discussion by Huffman's nephew</A>).
<P>
Finally, a reminder about notation.  If X is a random variable whose distribution
is (p<sub>1</sub>,p<sub>2</sub>,...,p<sub>k</sub>) then we usually
write H(X) for H(p<sub>1</sub>,p<sub>2</sub>,...,p<sub>k</sub>).  I will
occasionally write H(p) instead of H(X), where p abbreviates
p<sub>1</sub>,p<sub>2</sub>,...,p<sub>k</sub>.
<P>
<hr>

<ol>


<LI>
(Manning and Schuetze exercise 2.12) Show that the KL divergence
is not symmetric by giving an example of two distributions p and q
for which D(p||q) is not equal to D(q||p).  Give the two distributions
and show the explicit computation of the KL divergences.
<P>
Note that simple toy distributions (e.g. over {heads,tails}) are
boring to grade.  You'll earn less goodwill on this problem for the
boring kind of example, and more goodwill for an example that is
significantly interesting or amusing.
<P>

<LI> 
  <UL>
  <LI> a.  Here I'm largely asking you to go again through
    the horse race example from class, putting things in your own words to make sure you really get it. Recall that
       the distribution over horses h0-h7 is p:
     <pre>
	       h0     h1   h2     h3     h4     h5     h6     h7
	 p     1/2    1/4  1/8    1/16   1/64   1/64   1/64   1/64
     </pre>
     Recall that the encoding I gave is actually an
     <em>optimal</em> encoding for this distribution (one that
     uses the fewest possible number of bits on average, when communicating
     horse race outcome events).  For any distribution, an optimal binary
     representation of the events, in the sense of the smallest number of bits
     on average, can be created using <A
     HREF="http://en.wikipedia.org/wiki/Huffman_coding">Huffman
     coding</A>.  Here's the encoding from class:
     <pre>
	       h0     h1   h2     h3     h4     h5     h6     h7
      codeP    0      10   110    1110   111100 111101 111110 111111
     </pre>
     <P>
     Show, <em>without any numerical computations</em>, that if you
     use codeP, the expected value E<sub>p</sub>[length(h)] (i.e. the expected
     value of length(h) under distribution p) is equal to H(p), the
     entropy of the distribution.  (Partial credit for doing the two
     computations explicitly and confirming that you get the same number.)
      <P>
  <LI>
     b.  Imagine that p is the true distribution of outcomes when these
     horses race, but that we do a terrible job of estimating the probabilities
     and come up with the following model, q:
     <pre>
	       h0     h1   h2     h3     h4     h5     h6     h7
	 q     1/8    1/8  1/8    1/8    1/8    1/8    1/8    1/8
     </pre>
     Since the horses all have the same probability, the best code we
     can invent will obviously use the same number of bits for each
     horse.  This means that the smallest number of bits we can use is
     log<sub>2</sub>8 = 3 bits:
     <pre>
	       h0     h1   h2     h3     h4     h5     h6     h7
      codeQ   000    001  010    011    100    101    110    111
     </pre>
     Now imagine that we communicate lots of horse race outcomes using
     codeQ.  Encoding winning horses using codeQ, what is
     E<sub>p</sub>[length(h)]?
     <P>
  <LI> c. Use the previous two parts to this question, together with the definition
       of relative entropy (KL divergence), to explain and illustrate
       Manning and Schuetze's observation: "KL divergence is the
       average number of bits that are wasted by encoding events from
       a distribution p with a code based on a not-quite-right
       distribution q."
  <P>
  </UL>


<LI> Suppose I am upstairs with my two young kids, giving them a bath, and my wife, Becky, is downstairs enjoying a rare moment of rest, drinking a cup of tea and reading a book. (Note that this is not the usual situation. They’re active kids and bathtime is generally a damp two-parent job!) If Becky hears one of the kids wail, she’ll most likely take another sip of tea and continue reading. If she hears me wail, she’ll probably sprint upstairs to see what’s wrong. Explain Becky’s behavior in explicit information-theoretic terms. (That means you should be using concepts, terminology, and argumentation based on information theory.)
<P>

<LI> (Based loosely on Manning and Schuetze exercise 2.10) Compute three
probability distributions over unigrams: one for 
the Republican speeches in 
<A HREF="http://www.cs.cornell.edu/home/llee/data/convote.html">the congressional
speech corpus</A>, one for the Democrats, and one for
<A HREF="http://umiacs.umd.edu/~resnik/723/politics2009/obama_inaugural2009.txt">Obama's inaugural address of January 20, 2009</A>.
<P>
Obama's address will need to be tokenized, and you should
convert all text to lowercase in order to mitigate sparse data
problems.  Note that you'll still need to do something about zero
counts.  As the simplest option, you can smooth using Laplace ("add
one", Manning and Schuetze Section 6.2.2) to avoid zero, even though
in practice this is not considered the state of the art.  Or, slightly
better, you could use add-k for a different k, e.g. setting k to 1/|V|
where |V| is the size of the vocabulary. For simplicity, you can also
assume that the total vocabulary is the union of all the unigrams in
the three texts.
<P>
Here's a simple observation that might help you avoid writing a lot of
new code from scratch, allowing you to re-use work you've already done
in the previous assignment: if you have a bigram frequency table with
entries count(a,b), then the unigram counts are just the marginal
counts for a and b, i.e.
<pre>
  count(a) = sum_{b'} count(a,b')
  count(b) = sum_{a'} count(a',b)
</pre>
Best practice would be to smooth the bigram counts first, and then
compute the unigram counts as above.  If you smooth unigrams and
bigrams separately, there's no guarantee that summing over smoothed
bigram counts to compute the marginals will give you the same number
you get from smoothing unigrams separately. But this is a minor point
and for this assignment I'm fine with any fairly reasonable smoothing
as long as you say clearly what you did.
<P>
Questions.
<ul>
  <li> a. Treating the distribution for the Obama address distribution as p, compute
the KL divergence against q (Democratic speeches) and q' (Republican speeches).  What are D(p||q) and D(p||q')?  How well does this match what you expected them to be?  Explain your answer.
<P>
  <li> b. Compute D(q||q') and D(q'||q), i.e. see what happens when you compare Republican speeches with Democratic speeches in both directions.  What are these values?  How do they compare with D(p||q) and D(q||p), where you're looking at the congressional speeches versus Obama's address?  Suggest some reasons the comparisons might be turning out the way they are.
  <P>
  <li> c. What are the top 10 terms contributing to D(q||q')?  Suggest reasons these might be the top 10.
    <P>
  <li> d. Same as (c), but for D(q'||q).
  <P>
</ul>
  <P>

</ol>

<P>

<P>
<HR>
<P>
</BODY>
</HTML>
