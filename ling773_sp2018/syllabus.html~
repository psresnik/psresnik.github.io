<html>
<BASE  HREF="http://umiacs.umd.edu/~resnik/ling773_sp2018/index.html">
<title>Schedule of Topics</title>
<body bgcolor="#ffffff">
<FONT FACE="Arial, Helvetica, Geneva" SIZE=-1>
<hr>
<h1>Schedule of Topics</h1>
<hr>
<P>

<!-- <A HREF="midterm_guidance.html">Guidance for midterm studying</A> -->

This is the schedule of topics for
<A HREF="http://umiacs.umd.edu/~resnik/ling773_sp2018/index.html">
Computational Linguistics II, Spring 2018</A>.
<P>
In readings, "M&amp;S" refers to
Christopher D. Manning and Hinrich Schuetze,
<A HREF="http://cognet.mit.edu/library/books/view?isbn=0262133601">Foundations
of Statistical Natural Language Processing</A>.
The "other" column has optional links pointing either
to material you should already know (but might want to review), or to
related material you might be interested in.
Make sure to do your reading <strong>before</strong> the class where it is listed!<P>
<P>

<font color="red" size=>THIS SCHEDULE IS A WORK IN PROGRESS!  <BR>
In addition, some topic areas may take longer than expected, so keep
an eye on the online class discussions for "official"
dates.</font>

<P>
See <A HREF="https://talks.cs.umd.edu/lists/7">CL Colloquium Talks</A> for possible extra credit each week.


<center>
<table  border=1 cellpadding=4 bgcolor=white>
<tr>
 <th>Class</th>
 <th>Topic<BR></th>
 <th>Readings<A HREF="#readings">*</A></th>
 <th>Assignments</th>
 <th>Other</th>
</tr>

<tr>
<td>Jan 24</td>
<td>Course organization, semester plan; knowledge-driven and data-driven NLP<BR>
</td>
<td>M&S Ch 1, 2.1.[1-9] (for review)<BR>
<em>
  <!--COVER Historical overview; Zipf's law; Probability spaces; finite-state and Markov models; Bayes' Rule; Bayesian updating; conjugate priors-->
  <!-- Word counts; tokenization; frequency and Zipf's law; concordances -->
</em>
</td>
<td>

  <A HREF="http://umiacs.umd.edu/~resnik/ling773_sp2018/assignments/assignment1.html">Assignment 1</A>

  <P>
</td>
<td>
<A HREF="http://languagelog.ldc.upenn.edu/">Language Log</A> (<em>the</em> linguistics blog);
<A href="http://nlpers.blogspot.com/">Hal Daum&eacute;'s NLP blog</A> (excellent blog, often technical machine learning stuff, but just as often more general interest, make sure to read the comment threads also because they're often excellent)
  <!-- <A HREF="http://www.economist.com/science/displayStory.cfm?story_id=3576374">Corpus Colossal</A> (The Economist, 20 Jan 2005);
  <A HREF="http://umiacs.umd.edu/~resnik/temp/lsedemo_for_class.pdf">
  Resnik and Elkiss (DRAFT)</A>;  <A HREF="http://lse.umiacs.umd.edu/">Linguist's Search Engine</A> -->
</td>
</tr>

<tr>
<td>Jan 31</td>
<td>Lexical association measures and hypothesis testing<BR>
</td>
<td>M&S Ch 5<BR>
<em><!--COVER Collocations;  mutual information; hypothesis testing--></em>
</td>
<td>

  <A HREF="http://umiacs.umd.edu/~resnik/ling773_sp2018/assignments/assignment2.html">Assignment 2</A>

  <P>
<td> 
<A HREF="http://www.aclweb.org/anthology/J93-1003">Dunning (1993)</A> is a classic and valuable to read if you're trying to use mutual information or chi-squared and getting inflated values for low-frequency observations.
<A HREF="http://research.microsoft.com/pubs/68957/rare-events-final-rev.pdf">Moore (2004)</A> is a less widely cited but very valuable discussion about how to judge the significance of rare events.
<P>
A really important paper by Ionnidis about problems with statistical hypothesis testing is
<A HREF="http://www.plosmedicine.org/article/info:doi/10.1371/journal.pmed.0020124">Why Most Published Research Findings Are False</A>;
for a very readable discussion see <A HREF="http://www.economist.com/news/briefing/21588057-scientists-think-science-self-correcting-alarming-degree-it-not-trouble">Trouble at the Lab</A>, The Economist, Oct 19, 2013 and the really great <A HREF="http://www.economist.com/blogs/graphicdetail/2013/10/daily-chart-2">accompanying video</A>. (Show that one to your friends and family!) For an interesting response, see <A HREF="http://www.edwardtufte.com/files/Study2.pdf">Most Published Research Findings Are False--But a Little Replication Goes a Long Way"</A>.
<P>
<A HREF="http://www.kilgarriff.co.uk/Publications/2005-K-lineer.pdf">Kilgarriff (2005)</A> is a fun and contrarian read regarding the use of hypothesis testing methodology specifically in language research.
<P>
<em>Named entities</em> represent another form of lexical association.  Named entity recognition is introduced in <A HREF="http://www.cs.colorado.edu/~martin/csci5417/ie-chapter.pdf">Jurafsky and Martin, Ch 22</A> and <A HREF="http://www.nltk.org/book/ch07.html">Ch 7 of the NLTK book</A>.

  <!--<A HREF="http://www.linguistics.ucsb.edu/faculty/stgries/research/NHST_CLLT.pdf">Gries (2005)</A>;<BR>-->
  <!--  <A  HREF="http://bmj.bmjjournals.com/cgi/content/full/310/6973/170">Bland and Altman (1995)</A>;<BR> -->
</td>
</tr>


<tr>
<td>Feb 7</td>
<td>Information theory<BR>
</td>
<td>M&S Ch 2.2, M&S Ch 6<BR>
<P>
Piantadosi et al. (2011), <A HREF="http://www.pnas.org/content/108/9/3526.long">Word lengths are optimized for efficient communication</A>;
Jaeger (2010), <A HREF="http://dx.doi.org/10.1016/j.cogpsych.2010.02.002">Redundancy and reduction: Speakers manage syntactic information density</A>
<em><!--COVER Information theory essentials; entropy, relative entropy, mutual
    information; noisy channel model; cross entropy and perplexity-->

</em>
</td>
<td>

<A HREF="http://umiacs.umd.edu/~resnik/ling773_sp2018/assignments/assignment3.html">Assignment 3</A>

  <!--  <A
    HREF="http://umiacs.umd.edu/~resnik/ling773_sp2018/assignments/info_theory.html">Assignment 2
  -->
  </td>
</td>
<td>

<A HREF="http://www.amazon.com/Elements-Information-Theory-Telecommunications-Processing/dp/0471241954">
Cover and Thomas (1991)</A> is a great, highly readable introduction to information theory.  The first few chapters go into many concepts from this lecture with greater rigor but a lot of clarity.<P>
 Maurits et al. (2010), <A HREF="http://papers.nips.cc/paper/4085-why-are-some-word-orders-more-common-than-others-a-uniform-information-density-account">Why are some word orders more common than
others? A uniform information density account</A>.  See also the syllabus for a 2009 seminar taught by Dan Jurafsky and Michael Ramscar, <A HREF="http://www.stanford.edu/class/psych227/">Information-Theoretic Models of Language and Cognition</A>, which looks as if it was awesome.
<P>
Roger Levy provides <A HREF="http://idiom.ucsd.edu/~rlevy/papers/proof_info_density_optimize.pdf">a formal proof that uniform information density minimizes the "difficulty" of interpreting utterances</A>. The proof assumes that, for any given word i in an utterance, the difficulty of processing it is some power k of its surprisal with k &gt; 1.
</td>
</tr>

<tr>
<td>Feb 14</td>
<td>HMMs and Expectation Maximization<BR>
</td>
<td>Skim M&S Ch 9-10, Chapter 6 of <A HREF="http://www.umiacs.umd.edu/~jimmylin/book.html">Lin and Dyer</A>. Read
 <A HREF="http://umiacs.umd.edu/~resnik/ling773_sp2011/readings/em_recipe.v2.only_hmm.pdf">my EM recipe discussion.</A><BR>
<em><!--COVER Maximum likelihood estimation overview; quick review of smoothing; EM overview;
    interpolated estimation; Katz backoff; HMM as a noisy channel model
    HMM review; deriving forward-backward algorithm as an instance of EM; Viterbi algorithm review.-->
</em>
</td>
  <!-- <td><A  HREF="http://umiacs.umd.edu/~resnik/ling773_sp2018/assignments/smoothing.html">Assignment 3 </td> -->

<td>

<A  HREF="http://umiacs.umd.edu/~resnik/ling773_sp2018/assignments/assignment4.html">Assignment 4</A>

  <P>
<td>
Recommended reading (and code to look at!): Dirk Hovy's <A HREF="https://github.com/dirkhovy/emtutorial">Interactive tutorial on the Forward-Backward Expectation Maximization algorithm</A>. Note that although his <A HREF="https://github.com/dirkhovy/emtutorial/blob/master/An%20Evening%20with%20EM.ipynb">iPython notebook</A> is designed to be interactive, you can also simply read it.
<!--
 <A HREF="http://research.microsoft.com/~joshuago/tr-10-98.pdf">An
 empirical study of smoothing techniques for language modeling</A> (Stanley
 Chen and Joshua Goodman, Technical report TR-10-98, Harvard University,
 August 1998); -->
  <BR>
</td>
</tr>


<tr>
<td>Feb 21</td>
<td>Guest lecture (Han-Chin Shing): Reduced-dimensionality representations for words</td>
<td>
<A HREF="https://arxiv.org/pdf/1301.3781.pdf">Efficient Estimation of Word Representations in Vector Space</A>
(with a focus on the the network architecture of CBOW and SkipGram);
<A HREF="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf">Distributed Representations of Words and Phrases and their Compositionally</A>
(with a focus on hierarchical softmax and negative sampling);
<A HREF="http://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html">Deep Learning with PyTorch: A 60 Minute Blitz</A>
(a really good tutorial for PyTorch)
</td>
<td>
<A HREF="https://github.com/sidenver/word2vec_assignment">Assignment 5</A>
</td>
<td>
</td>
</tr>




<tr>
<td>Feb 28</td>
<td>Reduced-dimensionality representations for documents: Gibbs sampling and topic models</td>
<td>
Read Philip Resnik and Eric Hardisty, <A HREF="http://umiacs.umd.edu/~resnik/pubs/gibbs.pdf">Gibbs Sampling for the Uninitiated</A>; 
watch <A HREF="http://youtu.be/4p9MSJy761Y">Jordan Boyd-Graber's 2013 CL1 topic modeling lecture</A> (20 minutes, slides/notes available <A HREF="http://www.umiacs.umd.edu/~jbg/teaching/CMSC_723_2013/lecture_10.pdf">here</A>. 
<BR>
</td>
<td>
  <A HREF="http://www.umiacs.umd.edu/~resnik/ling773_sp2018/assignments/lda_assignment">Assignment 6</A>
  </td>
<td>
Recommended reading: Steyvers and T. Griffiths (2007), <A HREF="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.80.9625&rep=rep1&type=pdf">Latent Semantic Analysis: A Road to Meaning</A>. 
 <P>
</td>
</tr>



<tr>
<td>March 7</td>
<td>Context-free parsing
</td>
<td>
<!--
Supervised classification: M&S Ch 16 <em>except</em> 16.2.1;
Hearst et al. 1998 <A HREF="http://www.svms.org/tutorials/Hearst-etal1998.pdf">Support Vector Machines</A> (cleaner copy  <A HREF="https://web.archive.org/web/20130729024316/http://1024.ii.uni.wroc.pl/~aba/teach/SSNiS/CKpaper2.pdf">here</A>)<P>
-->
M&amp;S Ch 11 (esp. pp. 381-388) and Ch 12 (esp. pp. 408-423, 448-455)
 </td>
<td>
  <A HREF="http://www.umiacs.umd.edu/~resnik/ling773_sp2018/assignments/parsing.html">Assignment 7</A>.  
  <!-- This is a lighter assignment worth 50% of a regular homework.   -->
 </td>
<td>
A really nice article introducing parsing as inference is Shieber et al., <A HREF="http://www.eecs.harvard.edu/shieber/Biblio/Papers/infer.pdf">Principles and Implementation of Deductive Parsing</A> (see Sections 1-3), with a significant advance by Joshua Goodman, <A HREF="http://dl.acm.org/citation.cfm?id=973230">Semiring Parsing</A>.

<!--
Hearst et al. (1998) is a nice  SVM reading because it's the clearest, shortest possible introduction.  There are many other good things to read at <A HREF="http://www.svms.org/">svms.org</A>, including a "best tutorials" section, broken out by introductory, intermediate, and advanced, under <A HREF="http://www.svms.org/tutorials/">Tutorials</A>.   Feel free to go with one of the other tutorials (the ones I've seen used most often are <A HREF="">Burges 1998</A> and <A HREF="">Smola et al. (1999)</A>)) instead of Hearst if you want a meatier introduction.
-->
</td>
</tr>


<tr>
<td>Mar 14 </td>
<td>
Evaluation 
</td>
<td>
Lin and Resnik, <A HREF="https://web.archive.org/web/20130421055104/http://www.umiacs.umd.edu/~jbg/teaching/CMSC_773_2012/reading/evaluation.pdf">Evaluation of NLP Systems</A>, Ch 11 of Alex Clark, Chris Fox and Shalom Lappin, eds., Blackwell Computational Linguistics and Natural Language Processing Handbook. <BR>
Cohen and Howe, <A HREF="http://www.aaai.org/ojs/index.php/aimagazine/article/viewFile/952/870">How Evaluation Guides AI Research</A>
 </td>
<td>
<!-- <A HREF="assignments/assignment8.pdf">Assignment 8</A> (lighter assignment ahead of spring break) -->
</td>
<td>
See Pereira, <A HREF="http://rsta.royalsocietypublishing.org/content/roypta/358/1769/1239.full.pdf">Formal grammar and information theory: together again?</A> for discussion of probabilistic grammar and the argument made by Chomsky involving the sentences <em>Colorless green ideas sleep furiously</em>  and <em>Furiously sleep ideas green colourless</em>.
</td>
</tr>


<tr>
<td>Mar 21</td>
<td><font color="red">Spring Break</font> <BR>
</td>
<td></td>
<td>Have fun!</td>
<td></td>
</tr>


<tr>
<td>Mar 28</td>
<td>Guest lecture (Joe Barrow): Sequence and seq2seq models</td>
</td>
<td>No required readings</td>
<td><font color="red">Take-home midterm handed out, due 11:59pm Sunday April 1.</font></td>
<td></td>
</tr>



<!--
<tr>
<td>March 29 </td>
<td>
Structured prediction
</td>
<td>
Resources:
Noah Smith, <A HREF="http://www.cs.cmu.edu/~nasmith/LSP/">Linguistic structure prediction</A>;
Joshua Goodman, <A HREF="http://dl.acm.org/citation.cfm?id=973230">Semiring Parsing</A>
 </td>
<td>
Assignment?
</td>
<td>
</td>
</tr>
-->


<tr>
<td>April 4</td>
<td>Deep learning and linguistic structure: a broader perspective<P>
</td>
<td>
  Yoav Goldberg,  <A HREF="http://cs.biu.ac.il/~yogo/nnlp.pdf">A Primer on Neural Network Models for Natural Language Processing</A>. 
  Read with an emphasis on Sections 1-4, and Section 5 (embeddings); also look over 10-11 (RNNs) and 12 (recursive NNs) and the Wikipedia page on <A HREF="https://en.wikipedia.org/wiki/Autoencoder">autoencoders</A>.
  </td>
<td>
 <A HREF="">Project</A> handed out. Project plans due in one week.
</td>
<td>
Recommended: Sections 1, 3 and 4 of Yoshua Bengio, <A HREF="https://wiki.eecs.yorku.ca/course_archive/2012-13/F/6328/_media/learning-deep-ai.pdf">Learning Deep Architectures for AI</A>. Also recommended: the nice overview of representation learning in sections 1-4 of Bengio et al. <A HREF="http://arxiv.org/pdf/1206.5538v2.pdf">Representation Learning: A Review and New Perspectives</A>. 
<P>
Other useful background reading for broader perspective:
Lillian Lee, <A HREF="http://www.cs.cornell.edu/home/llee/papers/cf.pdf">Measures of Distributional Similarity</A>,
Hinrich Schuetze, <A HREF="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.41.8856">Word Space</A>,
Mikolov et al., <A HREF="http://research.microsoft.com/pubs/189726/rvecs.pdf">Linguistic Regularities in Continuous Space Word Representations</A>.
  </td>
</tr>


<tr>
<td>April 11</td>
<td>Machine translation<BR>
</td>
<td>

Koehn, <A HREF="http://www.statmt.org/book/">Statistical Machine Translation</A>;
Chiang, <A HREF="http://www.aclweb.org/anthology/P05-1033">A Hierarchical Phrase-Based Model for Statistical Machine Translation</A>; M&amp;S Ch 13 and Adam Lopez, <A HREF="http://homepages.inf.ed.ac.uk/alopez/papers/survey.pdf">Statistical Machine Translation</A>, ACM Computing Surveys  40(3), Article 8, pages 149, August 2008;
Wu et al. (2016), 
<A HREF="https://arxiv.org/abs/1609.08144">Google's neural machine translation system: Bridging the gap between human and machine translation</A>.
<!--
Noah Smith, <A HREF="http://www.cs.cmu.edu/~nasmith/LSP/">Linguistic Structure Prediction</A>, esp. Sections 3.5.2-3.7. (The book is available <A HREF="http://dx.doi.org/10.2200/S00361ED1V01Y201105HLT013">online</A> for UMD and many other university IP addresses.)  See also Noah Smith's <A HREF="http://www.cs.cmu.edu/~nasmith/slides/sp4nlp.icml09.pdf">Structured prediction for NLP</A> tutorial slides (ICML'09).
-->
</td>
<td>
Work on your project from here on out!
</td>
<td>
</td>
</tr>

<tr>
<td>April 18</td>
<td>Text analysis in computational social science</td>
<td></td>
<td></td>
<td>
<font size=-2>
<ul>
<LI> Stephan Greene and Philip Resnik, <A HREF="pubs/greene_resnik_naacl2009.pdf">More Than Words: Syntactic Packaging and Implicit Sentiment</A>, NAACL 2009, Boulder, CO, May 31 - June 5, 2009.
<LI> Viet-An Nguyen, Jordan Boyd-Graber, Philip Resnik, Deborah Cai, Jennifer Midberry, Yuanxin Wang, <A HREF="http://link.springer.com/article/10.1007%2Fs10994-013-5417-9">"Modeling topic control to detect influence in conversations using nonparametric topic models"</A>, <em>Machine Learning</em>, October 2013.
<LI> Viet-An Nguyen, Jordan Boyd-Graber, and Philip Resnik, <A HREF="http://papers.nips.cc/paper/5163-lexical-and-hierarchical-topic-regression">"Lexical and Hierarchical Topic Regression"</A>, NIPS 2013.
<LI> Mohit Iyyer, Peter Enns, Jordan Boyd-Graber, and Philip Resnik. <A HREF="http://www.aclweb.org/anthology/P/P14/P14-1105.pdf">Political Ideology Detection Using Recursive Neural Networks</A>. Association for Computational Linguistics, 2014.
<LI> Viet-An Nguyen, Jordan Boyd-Graber, Philip Resnik, and Kristin Miler. ``Tea Party in the House: A Hierarchical Ideal Point Topic Model and Its Application to Republican Legislators in the 112th Congress'', Association for Computational Linguistics Conference (ACL), Beijing, July, 2015.
</ul>
</font>
</td>
</tr>


<tr>
<td>April 25</td>
<td>Text analysis in computational social science, continued</td>
<td></td>
<td></td>
<td>
</td>
</tr>


<!--
<tr>
<td>April 25</td>
<td>Tentative: structured prediction<BR></td>
<td>
Noah Smith's (2004) <A HREF="http://www.cs.cmu.edu/~nasmith/papers/smith.tut04.pdf">Log-Linear Models</a> (selections);
Ke Wu, <a href="../ling773_sp2013/seq-label.pdf">Discriminative Sequence Labeling</a>; 
</td>
<td>
</td>
<td>
Historical background: Ratnaparkhi (1996), <A HREF="http://acl.ldc.upenn.edu/W/W96/W96-0213.pdf">A Maximum Entropy Model for Part of Speech Tagging</A>, or, if you want a little more detail, Ratnaparkhi (1997), <A HREF="http://repository.upenn.edu/cgi/viewcontent.cgi?article=1083&context=ircs_reports">A Simple Introduction to Maximum Entropy Models for Natural Language Processing</A>.  Ratnaparkhi 1996 began the popularization of maxent in NLP. 
<P>
For relevant background on semirings see Joshua Goodman, <A HREF="http://acl.ldc.upenn.edu/J/J99/J99-4004.pdf">Semiring Parsing</A>. For an example of very interesting recent work in this area particularly from an automata-theoretic angle, see Chris Dyer, <A HREF="http://ling.umd.edu/assets/publications/chris.dyer.diss.pdf">A Formal Model of Ambiguity and its Applications in Machine Translation</A>.
<P>
Also potentially of interest:
<ul>
<li> Hanna Wallach's <A HREF="http://www.inference.phy.cam.ac.uk/hmw26/papers/crf_intro.pdf">"Conditional Random Fields: An Introduction"</A>. 
<li> Charles Sutton and Andrew McCallum's <a href="http://homepages.inf.ed.ac.uk/csutton/publications/crftut-fnt.pdf">CRF tutorial</a> in 
Foundations and Trends of Machine Learning.
</ul>
<P>
Montreal's <A HREF="http://104.131.78.120/">LISA laboratory</A> is an epicenter for work on neural machine translation.
-->
<!--
<li> Ben Taskar et al.'s <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.101.6014&rep=rep1&type=pdf">dissertation</a> on large margin training of structured prediction models.
<li> <a href="http://luthuli.cs.uiuc.edu/~daf/courses/learning/StructureLearning/tsochantaridis05a.pdf">Another paper</a> by Ioannis Tsochantaridis et al. on the same topic with a different approach.
<li> A nice <a href="http://www.chokkan.org/software/crfsuite/benchmark.html">benchmark comparison</a> of several algorithms for training CRFs and structured perceptrons.
</ul>
-->

</td>
</tr>


<tr>
<td>May 2</td>
<td>TBD</td>
<td>
</td>
<td>
  </td>
<td>
</td>
</tr>

<tr>
<td>May 9</td>
<td>TBD</td>
<td>
</td>
<td>
Project is due 11:59pm ET May 18
</td>
<td></td>
</tr>
</table>
</center>

</table>
</center>


<!--
<tr>
<td>May 13</td>
<td>Make-up/Projects presentations? <BR></td>
<td>May 12 is the last day of classes.  However, I am reserving this date in case we wish to use it, e.g. for a make-up class if we have snow cancellations, or for final project presentations.   </td>
<td></td>
<td></td>
</tr>
-->

