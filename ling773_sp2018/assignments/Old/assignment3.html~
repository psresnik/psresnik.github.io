<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2//EN">
<HTML>
<HEAD>
   <TITLE>Information theory questions</TITLE>
</HEAD>
<BODY BGCOLOR="#FFFFFF">

<HR>
<H1>Information Theory Assignment</H1>
<HR>

<P>
First, a couple of notes.
<P>
For the mathematically inclined and/or those who liked the idea of
deriving an information measure from a set of properties such a measure should have,
here are <A
HREF="http://www.mtm.ufsc.br/~taneja/book/node5.html">several derivations</A> 
of the definition of entropy based on axioms about such
properties.
The one I mentioned in class is from A.I. Khinchin,
<em>Mathematical Foundations of Information Theory</em>, Dover, New
York, 1957.
<P>
Also, in case you're curious, here's the pointer to an algorithm for
creating optimal codes using <A
HREF="http://en.wikipedia.org/wiki/Huffman_coding">Huffman coding</A>
(see also this <A
HREF="http://web.archive.org/web/20110518080824/http://www.huffmancoding.com/my-family/my-uncle/huffman-algorithm">interesting
discussion by Huffman's nephew</A>).
<P>
Finally, a reminder about notation.  If X is a random variable whose distribution
is (p<sub>1</sub>,p<sub>2</sub>,...,p<sub>k</sub>) then we usually
write H(X) for H(p<sub>1</sub>,p<sub>2</sub>,...,p<sub>k</sub>).  I will
occasionally write H(p) instead of H(X), where p abbreviates
p<sub>1</sub>,p<sub>2</sub>,...,p<sub>k</sub>.
<P>
<hr>

<h2>Do BOTH of these on-paper problems</h2>
<ol>
<LI>
(Manning and Schuetze exercise 2.12) Show that the KL divergence
is not symmetric by giving an example of two distributions p and q
for which D(p||q) is not equal to D(q||p).  Give the two distributions
and show the explicit computation of the KL divergences. 
<P>
Note that if you understand KL-divergence, this problem should take you no more than 5 minutes, including writing it down. If it's taking you longer, go back and make sure you understand what KL-divergence is.
<P>
Also note that simple toy distributions (e.g. over {heads,tails}) are really boring to grade!  You can give us the boring kind of example, but you'll earn more goodwill for an example that is significantly interesting or amusing.
<P>

  
<LI> 
  <UL>
  <LI> a. Consider a set of eight horses, h0-h7, that repeatedly race
  each other. Suppose the probability distribution over horses h0-h7
  for winning a race is p:
     <pre>
	       h0     h1   h2     h3     h4     h5     h6     h7
	 p     1/2    1/4  1/8    1/16   1/64   1/64   1/64   1/64
     </pre>
     For any probability distribution, an optimal binary
     representation of the events, in the sense of requiring the smallest number of bits
     on average to communicate outcomes, can be created using <A
     HREF="http://en.wikipedia.org/wiki/Huffman_coding">Huffman
     coding</A>.  Here's the encoding you'd get for the above distribution:
     <pre>
	       h0     h1   h2     h3     h4     h5     h6     h7
      codeP    0      10   110    1110   111100 111101 111110 111111
     </pre>
     <P>
     Illustrate using codeP that the expected value of the length E<sub>p</sub>[length(h)] is equal to H(p), the
     entropy of the distribution. You can simply do the two
     computations numerically using codeP and confirm that you get the same number,
     though you'll understand what's going on better if you look at the definitions
     of E<sub>p</sub>[length(h)] and H(p) and show that one is equivalent to the other.
      <P>
  <LI>
     b.  Imagine that p is the true distribution of outcomes when these
     horses race, but that we do a terrible job of estimating the probabilities
     and come up with the following model, q:
     <pre>
	       h0     h1   h2     h3     h4     h5     h6     h7
	 q     1/8    1/8  1/8    1/8    1/8    1/8    1/8    1/8
     </pre>
     Since the horses all have the same probability, there's no basis
     for distinguishing them, so obviously the best code we
     can invent will use the same number of bits for each
     horse.  This means that the smallest number of bits we can use is
     log<sub>2</sub>8 = 3 bits:
     <pre>
	       h0     h1   h2     h3     h4     h5     h6     h7
      codeQ   000    001  010    011    100    101    110    111
     </pre>
     Now imagine that we communicate lots of horse race outcomes using
     codeQ.  Encoding winning horses using codeQ, what is
     E<sub>p</sub>[length(h)]?
     <P>
  <LI> c. Use the previous two parts to this question, together with the definition
       of relative entropy (KL divergence), to explain and illustrate
       Manning and Schuetze's observation: "KL divergence is the
       average number of bits that are wasted by encoding events from
       a distribution p with a code based on a not-quite-right
       distribution q."
  <P>
  </UL>
</ol>

<!--
  <LI> Suppose I am upstairs with my two young kids, giving them a bath, and my wife, Becky, is downstairs enjoying a rare moment of rest, drinking a cup of tea and reading a book. (Note that this is not the usual situation. They’re active kids and bathtime is generally a damp two-parent job!) If Becky hears one of the kids wail, she’ll most likely take another sip of tea and continue reading. If she hears me wail, she’ll probably sprint upstairs to see what’s wrong. Explain Becky’s behavior in explicit information-theoretic terms. (That means you should be using concepts, terminology, and argumentation based on information theory.) -->
  
<P>
<h2>Do ONE of the following TWO programming problems</h2>

<h3>Option 3a</h3>
Read Piantadosi et al. (2011), <A HREF="http://www.pnas.org/content/108/9/3526.long">Word lengths are optimized for efficient communication</A>, showing that word length correlates better with the average amount of information conveyed by a word in context than it does with the word's context-independent frequency or information.  (Wait, you mean you didn't <em>already</em> read it for last class?!)
This seems like it should be a really easy do-it-yourself experiment -- so the problem is to have a try at doing it yourself!
<P>
You can do this for the simple case of just bigram probabilities (i.e. context C=c means "the previous word is c"), although it would certainly also be interesting to see what things look like for higher-order ngrams. If you're doing that, I recommend using an existing language modeling module (e.g. in <A HREF="http://nltk.org">NLTK</A>) or toolkit (e.g. <A HREF="https://kheafield.com/code/kenlm/">KenLM</A> and <A HREF="http://www.speech.sri.com/projects/srilm/">SRILM</A>). As usual, just make sure to clearly identify any software that you didn't write yourself in your PDF writeup.  
<P>
You should do your experiment for <em>English and at least one other language</em>.
As one possible source of data to work with, see the <A HREF="http://nltk.github.com/nltk_data/">NLTK corpora</A>.  The <A HREF="http://www.nltk.org/nltk_data/packages/corpora/brown.zip">Brown corpus</A> would be a smallish but decent source for English (just remove POS tags and lowercase everything; it's already tokenized); you could also augment that with other English corpora, although be careful to make sure that your sources don't overlap, which would throw off your counts. For other languages, one source would be the NLTK samples from the Europarl corpora; if you were feeling more ambitious you could look into the  <A HREF="http://www.statmt.org/europarl/">full Europarl corpora</A>, which I believe are quite a bit larger. Or feel free to identify other sources of text. <em>The corpus processing (acquisition, cleanup, etc.) are not the point of this assignment, so I encourage people to share pointers, cleanup code, and even corpus data using Piazza. (Recall that constructive participation on Piazza contributes to your class participation grade.)</em>
<P>
<strong>You are <em>not</em> required to actually succeed in replicating the research results in the paper</strong> -- that would take a lot of work, since they used fairly sizeable corpora that you probably don't have easy access to.  In fact, you're not even required to compute correlations (though that would be nice); we'll be happy if you produce charts that look roughly like their Figure 2, no error bars required (though that would be nice also).<P>

<strong> What you do need to do</strong> involves (a) briefly summarizing what the study is about in your own words to show that you understand it, (b) clearly describing for us what you did in your own experiment (along with any simplifying choices you made and why/how you made them), and (c) clearly showing and telling us what you you found. If your results are very different from what you would have expected, then (d) suggest some explanations why (and ideally support your explanation with data). The more thoughtful and clearly written your writeup the better.
<P>
Remember that the goal is not to "get it right", but to show you understand the ideas and that you're taking a thoughtful approach. The writeup matters.
<P>
By the way, if you found this interesting, there's a follow-up paper in which <A HREF="https://arxiv.org/pdf/1703.01694">Meylan and Griffiths (2017)</A> argue that phonological distinctiveness actually outperforms word length as a predictor of frequency. They define the <em>phonological information content</em> of a word to measure
the distinctiveness of its phoneme-to-phoneme transitions. The intuition behind their argument is that it's not just a word's probability in context that influences how much effort it takes to recognize it -- it's also the number of competitors (alternative words) you could have been hearing at that point based on the acoustic evidence.

<P>
<h3>Option 3b</h3>
(Based very loosely on Manning and Schuetze exercise 2.10) For this problem, let's use the <A HREF="http://stateoftheunion.onetwothree.net/texts/stateoftheunion1790-2016.txt.zip">State of the Union(SOTU) addresses</A> of U.S. presidents, a collection of documents described in the previous assignment. For our purposes here, let's restrict ourselves to Presidents Bush-1 (that is, Republican president George Herbert Walker Bush), Clinton (Democrat), Bush-2 (that is, Republican president George W. Bush), and Obama (Democrat). For simplicity, you can assume that the total vocabulary is the union of all the unigrams in the various texts.
<P>
To answer the questions you'll need to estimate probability distributions over unigrams for each president. 
<blockquote>
Speeches will of course need to be tokenized, and you should
convert all text to lowercase in order to mitigate <A HREF="http://nlp.stanford.edu/~wcmac/papers/20050421-smoothing-tutorial.pdf">sparse data problems</A>.  Note that you'll need to do something about zero
counts.  As the simplest option, you can smooth using Laplace smoothing ("add
one", Manning and Schuetze Section 6.2.2) to avoid zero, even though
in practice this is not considered the state of the art. (Ok, I'm probably being overly generous. In <a href="http://nlp.stanford.edu/~wcmac/papers/20050421-smoothing-tutorial.pdf">these tutorial slides about smoothing</A> the author describes Laplace as "a horrible choice".)  Or, a bit better, you could use add-k for a different k, e.g. setting k to 1/|V| where |V| is the size of the vocabulary. That's really easy to implement, if you're programming from scratch, and it tends to work ok.  State of the art would be modified Kneser-Ney smoothing, though it would be a major distraction from the main point of the assignment for you to implement that from scratch.
<P>
Here's a simple observation that might help you avoid writing a lot of
new code from scratch, allowing you to re-use work you've already done
in the previous assignment: if you have a bigram frequency table with
entries count(a,b), then the unigram counts are just the marginal
counts for a and b, i.e.:
<P>
  count(a) = &sum;<sub>b'</sub> count(a,b')
<P>
  count(b) = &sum;<sub>a'</sub> count(a',b)
<P>
Best practice would be to smooth the bigram counts first, and then
compute the unigram counts as above.  If you smooth unigrams and
bigrams separately, there's no guarantee that summing over smoothed
bigram counts to compute the marginals will give you the same number
you get from smoothing unigrams separately. But this is a minor point
and for this assignment I'm fine with any fairly reasonable smoothing
as long as you say clearly what you did.
<P>
Speaking of implementing from scratch, you don't have to do everything completely from scratch. There are, for example, existing language modeling modules (e.g. in <A HREF="http://nltk.org">NLTK</A>) and toolkits (e.g. <A HREF="https://kheafield.com/code/kenlm/">KenLM</A> and <A HREF="http://www.speech.sri.com/projects/srilm/">SRILM</A>), and these days it's also not difficult to find code for neural network language models, e.g. <A HREF="https://github.com/pauldb89/OxLM">OxLM</A> or <A HREF="http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-2-implementing-a-language-model-rnn-with-python-numpy-and-theano/">this tutorial on implementing a recurrent neural network language model using Theano</A>; see also the CL1 notes on
<A HREF="http://www.cs.umd.edu/class/fall2017/cmsc723/slides/slides_07.pdf">n-gram language models</A> and
<A HREF="http://mt-class.org/jhu/slides/lecture-nn-lm.pdf">neural language models</A>.
You're welcome to use tools like those in any way you like -- just make sure to clearly identify any software that you didn't write yourself in your PDF writeup. And if you find a particularly good discussion/tutorial on the Web, please post it to the class discussion board!
<P>

</blockquote>
<P>
Questions.
<ul>
  <li> a. Treating the distribution for the Obama speeches as p, the Bush-2 speeches as q, and the Clinton speeches as q', what are D(p||q) and D(p||q')?  How well does this match what you expected them to be?  Explain your answer.
  <P>
  <li> b. What are the top 10 terms contributing to the KL divergence in both cases? Suggest reasons these might be the top 10. (If you don't find anything interesting in the top 10, you're welcome to look at more.)
  <P>
  <li> c. Build two unigram language models using the Bush-1 speeches and the Clinton speeches, and evaluate those two models using the Bush-2 speeches as the language L that represents "truth". What is the perplexity of each model, and which of the two is a better model for Bush-2's speeches? Does this fit with what you would have expected? Why or why not? 
  <P>
</ul>
  <P>


<P>
<h3>Extra credit (up to 10%)</h3>
<ul>

The idea for this extra credit is to do an analysis similar in spirit to the Civis Analytics blog posting, but exercising things you've learned in class. As long as you meet that goal, if you want to do something a little different from exactly what I'm suggesting here, because of your own interests, that's fine -- make it interesting!
  <P>
  
<!--
  <P>
<li> For the Convote corpus, analyze the language for one or more individual congressional debates, and identify
  bigrams that are associated with Yes or with No votes for that
  debate more often than you would expect by chance.  
  Describe what the debate is about (based on reading some of the
  speeches) and explain why the bigrams might be associated the way they
  are.
<P>
  --> 
<li> For all the <em>individual</em> SOTU speeches from Bush-2 to the present (inclusive), build a similarity matrix like the one in the Civis Analytics blog posting, e.g. here is the fragment for the Obama speeches:
<P>
  <IMG src="sotu_obama.png"</IMG>.
<P>
As your measure comparing any two speeches, use (1/2)*(D(p||q)+D(q||p)), where p and q are <strong>bigram distributions</strong> for the two speeches being compared. (Averaging the two KL-diverence values is a common way of producing a symmetric value. For related discussion see <A HREF="https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence">the Wikipedia page on Jensen-Shannon divergence</A>.)
The result can be just a big table of numbers, if you like, though if you want to do a nice-looking <A HREF="https://www.google.com/search?q=%22matrix+heatmap%22&safe=off&tbm=isch&tbo=u&source=univ&sa=X&ved=0ahUKEwjjgPXSivPKAhUI2SYKHe2XDxoQsAQIIw&biw=1440&bih=658">matrix heatmap</A> there are many packages for that, e.g. see <A HREF="https://plot.ly/python/heatmaps/">this one</A>.
<P>
a. Show the table that you constructed (or the heatmap).
<P>
b. Does the matrix look like you would have expected? Are there any surprises, and if so, what's your explanation? (If you want to explore what's going on in a data-driven way, you could take inspiration from question 3b, above.)
<P>
c. How do your table and analysis compare with what's in the Civis Analytics posting? Is what you're seeing generally similar? Any interesting differences? Again, do you have any thoughts about <em>why</em> you might or might not be seeing differences?
</ul>
<P>
<HR>
<P>
</BODY>
</HTML>
