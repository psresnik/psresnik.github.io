<!DOCTYPE HTML PUBLIC "-//IETF//DTD HTML//EN">
<html> <head>
<title>Exploring Topic Modeling</title>
</head>

<body>
<h1>Exploring Topic Modeling</h1>

<h2>Overview</h2>

In this assignment, you'll be exploring topic modeling. You can use whatever topic modeling package you'd like; some popular ones include <A HREF="http://mallet.cs.umass.edu/">MALLET</A>, <A HREF="https://radimrehurek.com/gensim/">gensim</A>, and <A HREF="http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html">scikit-learn</A>; also see <A HREF="https://github.com/trinker/topicmodels_learning">here</A> for various pointers with a leaning toward using the R programming language. The intent is <em>not</em> for you to implement a sampler for LDA yourselves.
<P>
<font color="red">If you have not already done so, please make sure you have done the readings for the previous lecture.</font> 
<P>
See <A HREF="./mallet_example.txt">here</A> for a quick example of how to run LDA and build a topic model using MALLET. It assumes there is a directory called <em>convote_train</em> containing .txt files one per document, where each .txt file is already whitespace-tokenized. (You can download that <A HREF="./convote_train.tar.gz">here</A>).  For example:
<font size="-2">
<pre>
mr. chairman , i thank the gentleman from texas ( mr. barton )  for yielding me this time .
the gentleman has done a magnificent job leading the committee on this new bill .
i would just say , in america we face some great challenges with regard to formulation of our energy policy .
the oil demand growth keeps rising due to the industrialization of the emerging world .
china consumes 7 million barrels per day ; and if china 's rise in world prominence is similar to that of korea and japan , china will consume 20 million barrels per day in less than 10 years .
the last big oil discovery was 30 years ago in the north sea .
china is trying to buy oil companies in canada ; india is trying to buy oil companies in russia ; the present world production capacity is 83 million barrels a day ; and we are running an estimated 81.5 million today , which means we are in the red zone .
the 14 largest oil fields in the world are 40 years old .
once they are taken out to 50 percent , water and fluids need to be pumped to keep production at existing levels .
we have some significant challenges .
support this bill .
</pre>
</font>
See MALLET documentation for an alternative input format where instead of a directory, there's a single input file with one document per line. FYI, MALLET's --output-topic-keys argument specifies a file showing the top words for each topic (i.e. the words at the top of the sorted topic-word probability distribution); the --output-doc-topics argument specifies a file where it will write the inferred document-topic distribution; and the --output-state file contains the final sampled assignment of topics to words.

<h2>What to do</h2>

You can use any of the following datasets, or feel free to suggest some other dataset that you'd like to use instead.
<ul>
<p><li> The Convote dataset from <A  HREF="http://www.umiacs.umd.edu/~resnik/ling773_sp2018/assignments/assignment2.html">Assignment 2</A>.
<p><li> The SOTU dataset from <A  HREF="http://www.umiacs.umd.edu/~resnik/ling773_sp2018/assignments/assignment2.html">Assignment 2</A>.
<p><li> Documents from <A HREF="./fcc_comments_text.zip">this random sample</A> extracted from <A HREF="https://sunlightfoundation.com/2014/09/02/what-can-we-learn-from-800000-public-comments-on-the-fccs-net-neutrality-plan/">a collection of 800,000 public comments on the Federal Communications Commission's net neutrality plan<A>. (There are six files and they are not equal in size; the smallest contains 1,582 comments and the largest contains 23,569.)  </ul>
  
<h3>Exploration 1: Trying out different numbers of topics.</h3>

As we discussed, LDA is an unsupervised learning algorithm. It is, more specifically, a <em>parametric</em> unsupervised learner, which here signifies that the key modeling parameter, the number of topics, is fixed in advance. In that respect it is very much like the unsupervised HMM you may have looked at in a previous assignment: you're going to get different behavior depending on the value you choose for the parameter.
<P>
As Steyvers and Griffiths (2007) discuss, there are a variety of ways to choose the number of topics. One of the simplest is simply to try out different numbers -- what happens when you use 5 topics? 10? 25? 30? 50? 100? 300?  Using whichever topic modeling package you decide to work with, try a range from small to larger numbers of topics, and report on what you find.  How does the collection of topics change?  If the package you use gives you a quality measure for the model (e.g. many provide the likelihood), how well does that value correspond with your subjective impression of the topics in terms of their quality, coherence, interpretability?
<P>
For those who are interested in learning more about evaluating topic models, a really groundbreaking paper was <A HREF="https://www.umiacs.umd.edu/~jbg/docs/nips2009-rtl.pdf">Chang et al. (2009)</A>'s "Reading Tea Leaves" paper, and an important follow-on, providing an automated approximation to Chang et al.'s procedure, is <A HREF="http://www.aclweb.org/anthology/E14-1056">Lau et al. (2014)</A>; there's code for doing that automatic evaluation <A HREF="https://github.com/jhlau/topic_interpretability">here</A>.

<h3>Exploration 2: The influence of phrases</h3>

Although LDA is a "bag of words" model and is often implemented using unigrams, there's good reason to think that this could potentially be suboptimal.  For example in a document with a phrase like <em>white house</em>, you probably don't want the words <em>white</em> and <em>house</em> to be treated the same as if they occurred in the sentence <em>John painted his house white</em>.
<P>
Recently Brendan O'Connor's group at UMass has been investigating the role of phrase detection for NLP tasks; their work can be found in Handler et al. (2016a), <A HREF="http://brenocon.com/oconnor_textasdata2016.pdf">A Little Bit of NLP Goes A Long Way: Adding Phrases to the Term-Document Matrix using Finite-State Shallow Parsing</A> and Handler (2016b), <A HREF="http://brenocon.com/handler2016phrases.pdf">Bag of What? Simple Noun Phrase Extraction for Text Analysis</A>, which includes an open-source implementation of their approach linked <A HREF="http://slanglab.cs.umass.edu/phrases/">here</A>. A key element of their argument is that shallow noun phrase detection is a good way to group phrases into tokens as a preprocessing step for many NLP tasks, including topic modeling.
<P>
Now, I have a conjecture: I think they may be oversimplifying by not distinguishing among two very different kinds of phrases.  Some phrases, like <em>white house</em>, <em>new york</em>, or <em>vice president</em>, are partially or mostly non-compositional. In those cases it feels to me like it makes sense to preprocess them into single tokens, because the
"whiteness" or "houseness" of the White House is probably not relevant in terms of the topics it's mentioned in, any more than the "newness" of New York is relevant to it as a geographic location.  (For example, White House can't be paraphrased "house that is white", vice president is not the same as "president who is associated with vice", etc.)
<P>
On the other hand, if you have a compositionally interpreted noun phrase like <em>hardware store</em>, the very fact of its compositionality means that the meanings of the individual words are still relevant: a hardware store is a store that sells hardware, and therefore <em>store</em> and <em>hardware</em> are independently relevant parts of its meaning.
<P>
Operationally, my conjecture suggests the following concrete question: instead of tokenizing <em>phrases</em> in preprocessing, is it perhaps better only to tokenize <em>non-compositional</em> phrases?
<P>
So, for this exploration, I'd like you to explore running topic models with different forms of preprocessing. Specifically, I'd like you to try running topic models with the following forms of preprocessing:
<P>
<ul>
  <li> Baseline (required): No phrases (same as Exploration 1)
</ul>
<P>
At least two of the following variations:
<ul>
  <p><li> Tokenize only named entities, e.g. turn <em>new york</em> into <em>new_york</em>
  <p><li> Tokenize syntactically identified noun phrases, e.g. <em>computer programmer</em> becomes <em>computer_programmer</em>. Here you might consider varying the sizes of the phrases, e.g. should <em>experienced computer programmer</em> be tokenized as a 3-word phrase, or should only 2-word phrases be allowed; if so, would a simple heuristic makes sense (e.g. take the ending bigram of a multi-word NP) or would a more sophisticated disambiguation strategy makes sense (e.g. choose bigram <em>computer programmer</em> because it's much more frequent than <em>experienced computer</em>)?
  <p><li> Tokenize statistically highly associated bigrams, which we've seen in the past is a decent first approximation for non- (or at least incomplete) compositionality. Here you should be able to easily re-use work you've done in earlier assignments. An interesting question is how to threshold: how associated is "associated enough" to consider a bigram worth tokenizing?
  <p><li> Combine the previous two ideas: use a syntactic method to filter down to syntactically relevant phrases, but then filter further by only considering a subset that is also highly associated and therefore (by hypothesis) less compositional.
  <p><li> I'm open to other ideas for testing my conjecture -- feel free to suggest some!
</ul>
<P>
In terms of tools, you might find <A HREF="http://spacy.io">spacy</A> useful since it identifies part of speech tags, noun phrases, and named entities (and even gives you dependency parses if you like).  There are various <A HREF="https://spacy.io/docs/usage/tutorials">tutorial introductions</A> that you might find useful, as well as good documentation on its <A HREF="https://spacy.io/usage/processing-pipelines">processing pipelines</A>.  Generally, I am open to people sending around pointers and helping each other out. I'm also open to people collaborating on this assignment; if you're interested in doing that contact me for guidelines. 

<P>
<h3>Extra credit possibilities (up to 10%)</h3>
<ul>
<p><li> In addition to evaluating topic model quality subjectively in the above explorations, use <A HREF="http://www.aclweb.org/anthology/E14-1056">Lau et al. (2014)</A>'s code for automatic topic quality evaluation <A HREF="https://github.com/jhlau/topic_interpretability">here</A>.
<p><li> Explore the influence of document collection size by doing one or both explorations using different-sized collections.  For example, the FCC collection would allow you to work with both smaller and larger subsets of the corpus.
<p><li> Explore including other kinds of syntactic relationship information as tokens, instead of just named entities and noun phrases. For example, what if you were to include verb-object (and/or other verb-argument) bigram pairs (e.g. adding a token <em>bought_house</em> given the input "he bought a big house")?
<p><li> Do the previous bullet but also explore the same compositionality question; for example, in <em>kick (the) bucket</em>, the independent meanings of <em>kick</em> and  <em>bucket</em> are not relevant so <em>kick_bucket</em> would be a good token, but in <em>elect (the) president</em> the words make independent contributions that would be lost if you tokenized this pair.
<p>
<p><li> Here's a research idea I haven't had the time to try but I think could be interesting and probably pretty easy. (If this one worked I could see a paper potentially coming out of it.) The dimensions in lexical vector space models like word2vec, and the topic distributions in LDA, are capturing different kinds of generalizations. In word2vec, <em>Congress</em> and <em>Politburo</em> (Communist Party executive committee) are quite similar because they appear in very similar local contexts (e.g. near the verb <em>decided</em>), but for most corpora one would expect them to appear in different topics, because they will tend to have different document-level co-occurrences (e.g. <em>Congress</em> appearing in documents about <underline>American</underline> politics with words like <em>Republican</em> and <em>Democrat</em>). There's been sophisticated modeling work on putting these two ideas together (e.g. <A HREF="http://cs.umd.edu/~wwyang/files/2017_emnlp_tree_prior.pdf">here</A> and <A HREF="https://github.com/weihua916/LCTM">here</A>) but I have a really simple idea that I believe has not been tried.
<P>
Researchers at CMU have developed <A HREF="http://www.cs.cmu.edu/~bmurphy/NNSE/">non-negative sparse embeddings</A>, which are similar to other kinds of embeddings you've seen (e.g. word2vec) except that the individual dimensions are more separable and interpretable (see <A HREF="http://talukdar.net/papers/naacl15_comp_nnse.pdf">Fyshe et al. (2015)</A>).  What if instead of using the words in a document themselves, we were to treat words as <underline>evidence for semantic properties</underline>, and do LDA representing documents as bags of semantic properties instead of as bags of words?  For example, the <A HREF="http://www.cs.cmu.edu/~bmurphy/NNSE/">non-negative sparse embeddings</A> page has a downloadable set of 300-dimensional NNSE word vectors based on syntactic dependencies (download <A HREF="http://www.cs.cmu.edu/~bmurphy/NNSE/depNNSE300.tab.zip">here</A>, vocabulary of 35K words), which tends to give you taxonomic/conceptual similarity rather than just relatedness (e.g. <em>racquet</em> and <em>bat</em> would be close together but <em>racquet</em> and <em>player</em> would not be). What if one were to choose a threshold (let's say 0.3 for the sake of this discussion) and say that the dimension or semantic property is "present" for the word if its weight in the vector is above that threshold?  In that case, if we had these NNSE word embeddings:
<pre> 
WORD      D1    D2    D3
apple     0.5   0.1   0.4
dessert   0.8   0.2   0.0
kitchen   0.0   0.8   0.2
table     0.0   0.6   0.4
windows   0.0   0.4   0.6
</pre> 
the LDA vocabulary would be the semantic properties {D1,D2,D3}, and using the threshold of 0.3 the original document <em>At my kitchen table I had an apple for dessert</em> would be transformed into the document <em>D2 D2 D1 D3 D1</em> (with semantic properties now being the "words" in the transformed document). The first and second D2 come from <em>kitchen</em> and <em>table</em>, respectively, then D1 and D3 from <em>apple</em>, and finally D1 from <em>dessert</em>. Notice that if we do this, we've used the information from the embeddings to provide LDA with evidence about the co-occurrence of semantic property D1 (things you eat) with semantic property D2 (domestic locations). One could try building an LDA topic model using these transformed representations of documents, and then explore what comes out.  For example, do the topics that you get, as characterized by their top "words" (=properties), capture any interesting generalities? Are they capturing something that word-based LDA isn't? Can they capture equally good generalizations with fewer documents?
<P>


</ul>


<h2>What to turn in</h2>

The main product for this assignment should be a thoughtful written discussion that includes what you did, why you did it, and what you found.  Please also turn in a .tar.gz or .zip file of your code.
<P>











<hr>
</body> </html>
