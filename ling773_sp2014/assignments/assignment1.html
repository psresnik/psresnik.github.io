<html>
<BASE  HREF="http://umiacs.umd.edu/~resnik/ling773_sp2011/assignments/assignment0.html">
<title>Assignment 1</title>
<body bgcolor="#ffffff">
<hr>
<h1>Assignment 1</h1>
<hr>
<P>

<hr>

<strong>Question 1.</strong>  In the car last year, my son (then four years old) generated 
the following utterance:
<pre>
You beeped her out of the way so we could wented to school.
</pre>
Well, yes, ok, I did in fact beep my horn at the car in front of me on our way to 
school.  Children tend to notice these things.
But the light was green, and she wasn't moving!  For the record, my 
horn beeping was what we call a "gentle tap", which is 
<A HREF="https://inlportal.inl.gov/portal/server.pt/gateway/PTARGS_0_1399_9854_0_0_18/5Communicating@theWheel.pdf">officially 
sanctioned, at least in Idaho</A>.
<P>
From an NLP perspective, my son's sentence is chock full of interesting stuff.  For each of the following terms or concepts,
(a) briefly define what the term means, preferably in your own words, (b) briefly identify where the concept is illustrated in this example, and 
(c) where relevant, comment on any ways in which this sentence might be particularly challenging for an NLP system.
<ul>
<li> Morphology
<li> Subcategorization
<li> Syntactic ambiguity
<li> Clause
<li> Word sense ambiguity
<li> Coreference
</ul>

The point of this question is to make sure that you've at least encountered these concepts, even if you have to look them up in the index 
of Manning &amp; Schuetze or on Wikipedia.  Most or all of them should be familiar to anyone who has taken CompLing1 or equivalent.
<em>Don't spend more than 5 minutes on any one of these</em>.  And, again, it's ok for people
to work together and discuss things, as long as you're telling me explicitly who deserves credit for what.  If you're not a native speaker
of English, I strongly encourage you to talk with one.

<P>
<hr>
<P>
<strong>Question 2.</strong> Recall that Bayes's Rule is often used in "Bayesian
updating", since it is used to take a <em>prior</em> probability
distribution and update it on the basis of observed evidence to
produce a <em>posterior</em> distribution.  Before, after, hence
updating; get it?
<P>
This notion of "updating" is particularly useful when applied
iteratively.  Consider a scenario where we have a prior distribution
P(H) over hypotheses, and then we observe evidence E1 and 
use Bayes's Rule to update our distribution to
<pre>
               P(E1 | H) P(H)
  P(H | E1) =  --------------
                 P(E1)
</pre>
The distribution P(H|E1) can now be viewed as a new, more informed
prior.  Therefore if we see a new piece of evidence E2, we can use
Bayes's Rule again to derive a new posterior.  Since P(H|E1) is the
prior now, the result will take
the form
<pre>
                  X  P(H | E1)
  P(H | E1,E2) =  ------------
                       Y
</pre>
<P>
Derive the simplest possible form of the full expression, i.e. 
fill in X and Y, assuming that E1 and E2 are independent.

<P>
<HR>

<strong>Question 3.</strong> First, if necessary, review Manning &amp; Schuetze Section 2.1, on 
foundational concepts in probability theory.
<P>
Then, go to <A HREF="http://www.wolframalpha.com/">Wolfram Alpha</A>
and try typing the following things into the box at the top (and then hit Enter or click the "=" sign).
<ul>
<li> BinomialDistribution[10, 0.5]
<li> BinomialDistribution[100, 0.5]
</ul>
Fun, huh?  Ok, now try it with the Beta distribution:
<ul>
<li> BetaDistribution[1,1]
<li> BetaDistribution[2,2]
<li> BetaDistribution[10,10]
<li> BetaDistribution[2,10]
<li> BetaDistribution[10,2]
</ul>
<!-- http://www.fortunecity.co.uk/meltingpot/back/340/product/java/cdfdemomain.html -->

<P>
<strong>3(a)</strong> Look at the PDFs (probability density functions)
for each one, and explain in plain English what each of these Beta
distributions represents, when interpreted as a Bayesian prior for the
probability of heads. (To keep things standard, let's all assume that
the prior belief is about the probability of heads for a
possibly-unfair coin.)  What is your expectation about what the probability of heads
will be, and how strongly do you think so?
<P>
<strong>3(b)</strong> Let's take the notion of "expectation" in (3a) a
little more literally, as in <em>expected value</em>, also known as
the <em>mean</em> of the distribution.  Try typing
BetaDistribution[&alpha;,&beta;] for some other values of &alpha; and
&beta;, and look at the number Wolfram Alpha reports as the "mean".
Can you figure out what the formula for the mean is, as a function of
&alpha; and &beta;?  (If you can't give an exact formula, at least
comment on some properties of the mean.  If you already knew the
formula, it's ok to tell me so.  But <em>don't</em> just look up the 
mean of the Beta distribution and copy it down; what would you learn from that?!.)  </ul>

<!-- <P>
<strong>Question 2 (10 points extra credit):</strong>  Read <A
HREF="http://en.wikipedia.org/wiki/Conjugate_prior">the Wikipedia page
describing conjugate priors</A>.  Suppose that we have a coin
parameterized by some unknown success probability <em>q</em> (= the probability of heads),
and we do an experiment where we flip the coin 10 times and get heads
7 times.
<ul>

<li> (a) Suppose we start by believing it's a fair coin, with a prior
  distribution P(q) = Beta(1,1) (that is, the Beta distribution with
  parameters &alpha;=1 and &beta;=1).  What is our posterior
  probability that it's a fair coin (i.e. the posterior probability
  of q=0.5) after we've observed the evidence provided by the
  experiment?
  <P>

<li> (b) Suppose we started by believing it's a fair coin, but with a
  prior distribution P(q) = Beta(10,10) (that is, the Beta
  distribution with parameters &alpha;=10 and &beta;=10).  With this
  different prior, what is our posterior probability that it's a fair
  coin after we've observed the evidence provided by the experiment?
  <P>

<li> (c) The Beta distribution gives the highest probability to q=0.5
  when &alpha;=&beta;, so in both (a) and (b) we started with a prior
  belief that it's a fair coin.  (You might find it fun and
  informative to play with <A
  href="http://www.cs.ubc.ca/~poole/demos/bayeslearn/beta.html">this
  applet</a> to see how the Beta distribution changes as &alpha; and
  &beta; vary.) So, in plain English, how is what we assumed as a
  prior in (b) different from what we assumed in (a), and what does
  this say about the relationship between the prior probability and
  the evidence we see?
  <P>

</ul>

Please show how you came up with your answers.
Feel free to use on-line calculators for B, e.g. <A
HREF="http://www.danielsoper.com/statcalc/calc35.aspx">this one</A> or
or <A
HREF="http://functions.wolfram.com/webMathematica/FunctionEvaluation.jsp?name=Beta">this
one</A>.  
-->

<hr>

</body>
</html>
