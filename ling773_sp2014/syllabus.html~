<html>
<BASE  HREF="http://umiacs.umd.edu/~resnik/ling773_sp2014/index.html">
<title>Schedule of Topics</title>
<body bgcolor="#ffffff">
<FONT FACE="Arial, Helvetica, Geneva" SIZE=-1>
<hr>
<h1>Schedule of Topics</h1>
<hr>
<P>

<!-- <A HREF="midterm_guidance.html">Guidance for midterm studying</A> -->

This is the schedule of topics for
<A HREF="http://umiacs.umd.edu/~resnik/ling773_sp2014/index.html">
Computational Linguistics II, Spring 2014</A>.
<P>
In readings, "M&S" refers to
Christopher D. Manning and Hinrich Schuetze,
<A HREF="http://cognet.mit.edu/library/books/view?isbn=0262133601">Foundations
of Statistical Natural Language Processing</A>.
The "other" column has optional links pointing either
to material you should already know (but might want to review), or to
related material you might be interested in.
Make sure to do your reading <strong>before</strong> the class where it is listed!<P>
<P>


<font color="red" size=>THIS SCHEDULE IS A WORK IN PROGRESS!  <BR>
In addition, some topic areas may take longer than expected, so keep
an eye on the online class discussions for "official"
dates.</font>

<center>
<table  border=1 cellpadding=4 bgcolor=white>
<tr>
 <th>Class</th>
 <th>Topic<BR></th>
 <th>Readings<A HREF="#readings">*</A></th>
 <th>Assignments</th>
 <th>Other</th>
</tr>

<tr>
<td>Jan 29</td>
<td>Course administrivia, semester plan; some statistical NLP fundamentals<BR>
</td>
<td>M&S Ch 1, 2.1.[1-9] (for review)<BR>
<em>
  <!--COVER Historical overview; Zipf's law; Probability spaces; finite-state and Markov models; Bayes' Rule; Bayesian updating; conjugate priors-->
  <!-- Word counts; tokenization; frequency and Zipf's law; concordances -->
</em>
</td>
<td>
  <A HREF="http://umiacs.umd.edu/~resnik/ling773_sp2014/assignments/assignment1.html">Assignment 1</A>
</td>
<td>
<A HREF="http://languagelog.ldc.upenn.edu/">Language Log</A> (<em>the</em> linguistics blog),
<A href="http://nlpers.blogspot.com/">Hal Daum&eacute;'s NLP blog</A> (excellent blog, often technical machine learning stuff, but just
  as often more general interest)
  <!-- <A HREF="http://www.economist.com/science/displayStory.cfm?story_id=3576374">Corpus Colossal</A> (The Economist, 20 Jan 2005);
  <A HREF="http://umiacs.umd.edu/~resnik/temp/lsedemo_for_class.pdf">
  Resnik and Elkiss (DRAFT)</A>;  <A HREF="http://lse.umiacs.umd.edu/">Linguist's Search Engine</A> -->
</td>
</tr>

<tr>
<td>Feb 5</td>
<td>Words and lexical association<BR>
</td>
<td>M&S Ch 5<BR>
<em><!--COVER Collocations;  mutual information; hypothesis testing--></em>
</td>
<td>
  <A HREF="http://umiacs.umd.edu/~resnik/ling773_sp2014/assignments/assignment2.html">Assignment 2</A>
<td> <A HREF="http://acl.ldc.upenn.edu/J/J93/J93-1003.pdf">Dunning (1993)</A> is a classic and valuable to read if you're trying to use mutual information or chi-squared and getting inflated values for low-frequency observations.
<A HREF="http://research.microsoft.com/pubs/68957/rare-events-final-rev.pdf">Moore (2004)</A> is a less widely cited but very valuable discussion about how to judge the significance of rare events.
<P>
A really important paper by Ionnidis about problems with statistical hypothesis testing is
<A HREF="http://www.plosmedicine.org/article/info:doi/10.1371/journal.pmed.0020124">Why Most Published Research Findings Are False</A>;
for a more recent and ery readable discussion see <A HREF="http://www.economist.com/news/briefing/21588057-scientists-think-science-self-correcting-alarming-degree-it-not-trouble">Trouble at the Lab</A>, The Economist, Oct 19, 2013 and the really great <A HREF="http://www.economist.com/blogs/graphicdetail/2013/10/daily-chart-2">accompanying video</A>.
<P>
<A HREF="http://www.kilgarriff.co.uk/Publications/2005-K-lineer.pdf">Kilgarriff (2005)</A> is a fun and contrarian read regarding the use of hypothesis testing methodology specifically in language research.
  <!--<A HREF="http://www.linguistics.ucsb.edu/faculty/stgries/research/NHST_CLLT.pdf">Gries (2005)</A>;<BR>-->
  <!--  <A  HREF="http://bmj.bmjjournals.com/cgi/content/full/310/6973/170">Bland and Altman (1995)</A>;<BR> -->
<P>
<em>Named entities</em> represent another form of lexical association.  Named entity recognition is introduced in <A HREF="http://www.cs.colorado.edu/~martin/csci5417/ie-chapter.pdf">Jurafsky and Martin, Ch 22</A> and <A HREF="http://www.nltk.org/book/ch07.html">Ch 7 of the NLTK book</A>.
</td>
</tr>


<tr>
<td>Feb 12</td>
<td>Information theory<BR>
</td>
<td>M&S Ch 2.2, M&S Ch 6<BR>
<P>
Optional: Piantadoso et al. (2011), <A HREF="http://www.pnas.org/content/108/9/3526.long">Word lengths are optimized for efficient communication</A>;
Jaeger (2010), <A HREF="http://dx.doi.org/10.1016/j.cogpsych.2010.02.002">Redundancy and reduction: Speakers manage syntactic information density</A>
<em><!--COVER Information theory essentials; entropy, relative entropy, mutual
    information; noisy channel model; cross entropy and perplexity-->

</em>
</td>
<td>
<A HREF="http://umiacs.umd.edu/~resnik/ling773_sp2014/assignments/assignment3.html">Assignment 3</A>
  <!--  <A
    HREF="http://umiacs.umd.edu/~resnik/ling773_sp2014/assignments/info_theory.html">Assignment 2
  -->
  </td>
</td>
<td>
<A HREF="http://www.amazon.com/Elements-Information-Theory-Telecommunications-Processing/dp/0471241954">
Cover and Thomas (1991)</A> is a great, highly readable introduction to information theory.  The first few chapters go into all of the concepts from this lecture with greater rigor but a lot of clarity.<P>
 Maurits et al. (2010), <A HREF="http://books.nips.cc/papers/files/nips23/NIPS2010_0369.pdf">Why are some word orders more common than
others? A uniform information density account</A>.  See also the syllabus for a 2009 seminar taught by Dan Jurafsky and Michael Ramscar, <A HREF="http://www.stanford.edu/class/psych227/">Information-Theoretic Models of Language and Cognition</A>, which looks as if it was awesome.


<P>

</td>
</tr>

<tr>
<td>Feb 19</td>
<td>Maximum likelihood estimation and Expectation Maximization<BR>
</td>
<td>Skim M&S Ch 9-10, Chapter 6 of <A HREF="http://www.umiacs.umd.edu/~jimmylin/book.html">Lin and Dyer</A>. Read
 <A HREF="http://umiacs.umd.edu/~resnik/ling773_sp2011/readings/em_recipe.v2.only_hmm.pdf">my EM recipe discussion.</A><BR>
<em><!--COVER Maximum likelihood estimation overview; quick review of smoothing; EM overview;
    interpolated estimation; Katz backoff; HMM as a noisy channel model
    HMM review; deriving forward-backward algorithm as an instance of EM; Viterbi algorithm review.-->
</em>
</td>
  <!-- <td><A  HREF="http://umiacs.umd.edu/~resnik/ling773_sp2014/assignments/smoothing.html">Assignment 3 </td> -->

<td><A  HREF="http://umiacs.umd.edu/~resnik/ling773_sp2014/assignments/assignment4.html">Assignment 4</A></td>
<td>
<!--
 <A HREF="http://research.microsoft.com/~joshuago/tr-10-98.pdf">An
 empirical study of smoothing techniques for language modeling</A> (Stanley
 Chen and Joshua Goodman, Technical report TR-10-98, Harvard University,
 August 1998); -->
  <BR>
</td>
</tr>


<tr>
<td>Feb 26</td>
<td>Bayesian inference and modeling<P>
Overview of final exam project
</td>
<td>Philip Resnik and Eric Hardisty, <A HREF="http://umiacs.umd.edu/~resnik/pubs/gibbs.pdf">Gibbs Sampling for the Uninitiated</A>.  No need to work through all the equations in Section 2 in detail, but read carefully enough to understand the concepts. <P>
Read M. Steyvers and T. Griffiths (2007), <A HREF="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.80.9625&rep=rep1&type=pdf">Latent Semantic Analysis: A Road to Meaning</A> <em>and/or</em> review the CL1 topic modeling lecture (<A HREF="http://www.umiacs.umd.edu/~jbg/teaching/CMSC_723_2013/lecture_10.pdf">notes</A>, <A HREF="http://youtu.be/4p9MSJy761Y">video</A>).
<BR>
<em>
  <!--COVER [tentative]
  Graphical model representations of generative models; MLE, MAP, and Bayesian inference; Markov Chain Monte Carlo (MCMC)and Gibbs Sampling;
  Latent Dirichlet Allocation (LDA)-->
</em>
</td>
<td>
Do one of EC1, EC2, or EC3 from <A HREF="assignments/assignment4.html">Assignment 4</A>.
(Worth 50% of a usual homework, and not due until <em>4:30pm Friday March 7</em>)
  </td>
<td>
For a very nice and brief summary of LDA, including a really clear explanation of the corresponding Gibbs sampler (with pseudocode!), see Section 5 of Gregor Heinrich, <A HREF="http://faculty.cs.byu.edu/~ringger/CS601R/papers/Heinrich-GibbsLDA.pdf">Parameter estimation for text analysis</A>.
<P>
I will touch on supervised topic models, particularly in the context of the project; I recommend reading Blei and McAuliffe, <A HREF="https://www.cs.princeton.edu/~blei/papers/BleiMcAuliffe2007.pdf">Supervised Topic Models</A> (though note that we will not be talking about variational EM).  Also relevant is Nguyen, Boyd-Graber, and Resnik, <A HREF="http://www.umiacs.umd.edu/~jbg/docs/2013_shlda.pdf">Lexical and Hierarchical Topic Regression</A>.
<P>
If you're interested in going back to the source for LDA, see Blei, Ng, and Jordan (2003), <A HREF="http://www.cs.princeton.edu/~blei/papers/BleiNgJordan2003.pdf">Latent Dirichlet Allocation</A>.  
</td>
</tr>


<tr>
<td>Mar 5</td>
<td>Supervised classification<BR></td>
<td>M&S Ch 16 <em>except</em> 16.2.1;
Hearst et al. 1998 <A HREF="http://www.svms.org/tutorials/Hearst-etal1998.pdf">Support Vector Machines</A> (cleaner copy  <A HREF="https://web.archive.org/web/20130729024316/http://1024.ii.uni.wroc.pl/~aba/teach/SSNiS/CKpaper2.pdf">here</A>)
 <BR>
  <em>
   <!--COVER Supervised learning -- k-nearest neighbor classification;
   naive Bayes; decision lists; decision trees; transformation-based
   learning (Sec 10.4); linear classifiers; the kernel trick;
   perceptrons; SVM basics. -->
</em>
</td>
<td>Assignment 7 (?)</td>
<td>
I picked Hearst et al. (1998) as the SVM reading because it's the clearest, shortest possible introduction.  There are many other good things to read at <A HREF="http://www.svms.org/">svms.org</A>, including a "best tutorials" section, broken out by introductory, intermediate, and advanced, under <A HREF="http://www.svms.org/tutorials/">Tutorials</A>.   Feel free to go with one of the other tutorials (the ones I've seen used most often are <A HREF="">Burges 1998</A> and <A HREF="">Smola et al. (1999)</A>)) instead of Hearst if you want a meatier introduction.
<P>
Optional: Ratnaparkhi (1996), <A HREF="http://acl.ldc.upenn.edu/W/W96/W96-0213.pdf">A Maximum Entropy Model for Part of Speech Tagging</A>, or, if you want a little more detail, Ratnaparkhi (1997), <A HREF="http://repository.upenn.edu/cgi/viewcontent.cgi?article=1083&context=ircs_reports">A Simple Introduction to Maximum Entropy Models for Natural Language Processing</A>.  Ratnaparkhi 1996 began the popularization of maxent in NLP.  Noah Smith's (2004) <A HREF="http://www.cs.cmu.edu/~nasmith/papers/smith.tut04.pdf">Log-Linear Models</a> is a nice alternative introduction  expressed in a vocabulary that is more consistent with current work.
<P>
</tr>


<tr>
<td>Mar 12</td>
<td>Deep learning<BR>
</td>
<td>
Read sections 1, 3 and 4 of Yoshua Bengio, <A HREF="https://wiki.eecs.yorku.ca/course_archive/2012-13/F/6328/_media/learning-deep-ai.pdf">Learning Deep Architectures for AI</A>.  
Other sources we are likely to discuss include:
Lillian Lee, <A HREF="http://www.cs.cornell.edu/home/llee/papers/cf.pdf">Measures of Distributional Similarity</A>,
Hinrich Schuetze, <A HREF="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.41.8856">Word Space</A>,
Mikolov et al., <A HREF="http://research.microsoft.com/pubs/189726/rvecs.pdf">Linguistic Regularities in Continuous Space Word Representations</A>.
 </td>
<td></td>
<td>
Recommended: the nice overview of representation learning in sections 1-4 of Bengio et al. <A HREF="http://arxiv.org/pdf/1206.5538v2.pdf">Representation Learning: A Review and New Perspectives</A>, and the background on the skip-gram approach in <A HREF="https://code.google.com/p/word2vec/">word2vec</A> found in Mikolov et al., <A HREF="http://arxiv.org/pdf/1310.4546.pdf">Distributed Representations of Words and Phrases and their Compositionality</A>.  Background on Mikolov et al.'s Linguistic Regularities paper is in Mikolov et al. <A HREF="http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf">Recurrent neural network based language model</A>.
</td>
</tr>



<tr>
<td>Mar 19</td>
<td><font color="red">Spring Break</font> <BR>
</td>
<td></td>
<td>Have fun!</td>
<td></td>
</tr>

<tr>
<td>March 26 </td>
<td>Evaluation in NLP</td>
<td>Lin and Resnik, <A HREF="https://web.archive.org/web/20130421055104/http://www.umiacs.umd.edu/~jbg/teaching/CMSC_773_2012/reading/evaluation.pdf">Evaluation of NLP Systems</A>, Ch 11 of
  Alex Clark, Chris Fox and Shalom Lappin, eds., Blackwell Computational Linguistics and Natural
  Language Processing Handbook.<BR>
  <em><!--COVER Evaluation paradigms for NLP; parser evaluation --></em></td>
<td><font color="red">Take-home midterm</A></td>
<td></td>
</tr>



<tr>
<td>April 2</td>
<td>Structured learning and parsing</td>
</td>
<td>
<a href="../ling773_sp2013/seq-label.pdf">Discriminative Sequence Labeling</a> by Ke Wu
<P>
Noah Smith's <A HREF="http://www.cs.cmu.edu/~nasmith/slides/sp4nlp.icml09.pdf">Structured prediction for NLP</A> tutorial slides (ICML'09); 

</td>
<td></td>
<td>
Also of interest:
<ul>
<li> Hanna Wallach's <A HREF="http://www.inference.phy.cam.ac.uk/hmw26/papers/crf_intro.pdf">"Conditional Random Fields: An Introduction"</A>. 
<li> Charles Sutton and Andrew McCallum's <a href="http://homepages.inf.ed.ac.uk/csutton/publications/crftut-fnt.pdf">CRF tutorial</a> in Foundations and Trends of Machine Learning.
<li> Ben Taskar et al.'s <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.101.6014&rep=rep1&type=pdf">dissertation</a> on large margin training of structured prediction models.
<li> <a href="http://luthuli.cs.uiuc.edu/~daf/courses/learning/StructureLearning/tsochantaridis05a.pdf">Another paper</a> by Ioannis Tsochantaridis et al. on the same topic with a different approach.
<li> A nice <a href="http://www.chokkan.org/software/crfsuite/benchmark.html">benchmark comparison</a> of several algorithms for training CRFs and structured perceptrons.
</ul>
</td>
</tr>



<!--
<tr>
<td>March 27</td>
<td>More on supervised learning: maximum entropy models and conditional random fields <font color="red">[Guest lecturer TBA]</font><BR>
</td>
<td>
  <A HREF="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.63.2111">
   Using maximum entropy for text classification (Kamal Nigam, John Lafferty, Andrew McCallum)</A>;
  <A HREF="http://www.aclweb.org/anthology/N/N03/N03-1028.pdf">
   Shallow Parsing with Conditional Random Fields (Fei Sha and Fernando Pereira)</A>
<BR>
 <em>The maximum entropy principle;
     maxent classifiers (for predicting a single variable);
      CRFs (for predicting interacting variables);
      L2 regularization.
  </td>
<td></td>
</td>
<td>
   Optionally, some good introductory material appears in
   <A  HREF="http://www-2.cs.cmu.edu/afs/cs/user/aberger/www/html/tutorial/tutorial.html">
   Adam Berger's maxent tutorial</A>,
   Dan Klein and Chris Manning's
   <A HREF="http://www.cs.berkeley.edu/~klein/papers/maxent-tutorial-slides.pdf">
   Maxent Models, Conditional Estimation, and Optimization, without the Magic</A>, and
   Noah Smith's  <A HREF="http://umiacs.umd.edu/~resnik/ling848_fa2004/slides/loglinear_handout.pdf">
   notes on loglinear models</A> (which provides explicit details for a lot of the math).
   Another useful reading, focused on estimating the parameters of maxent models, is
   <A HREF="http://bulba.sdsu.edu/~malouf/papers/conll02.pdf">A comparison of algorithms for maximum
   entropy parameter estimation (Rob Malouf)</A>.
   Also, Manning and Schuetze section 16.2 can be read as supplementary material.
   Of historical interest:
   Adwait Ratnaparkhi's <A
   HREF="ftp://ftp.cis.upenn.edu/pub/ircs/tr/97-08.ps.Z">A Simple
   Introduction to Maximum Entropy Models for Natural Language
   Processing</A> (1997).
</td>
</tr>
-->


<tr>
<td>Apr 9</td>
<td>More on structured learning and parsing; Semi-supervised learning</td>
<td>TBD</td>
<td>TBD</td>
<td>TBD</td>
</tr>



<tr>
<td>April 16</td>
<td>Guest lecture: Doug Oard on a topic related to information retrieval </td>
<td></td>
<td>None.</td>
<td></td>
</tr>



<!--
<tr>
<td>Apr 22
<td>Information retrieval; guest lecture (Smaranda Muresan) on graph-based methods in NLP<BR>
</td>
<td>
(a) Rada Mihalcea and Paul Tarau, <A
HREF="http://www.cs.unt.edu/~rada/papers/mihalcea.emnlp04.pdf">TextRank:
Bringing Order into Texts</A>, in Proceedings of the Conference on
Empirical Methods in Natural Language Processing (EMNLP 2004),
Barcelona, Spain, July 2004.;
(b) Rada Mihalcea, <A
HREF="http://www.cs.unt.edu/~rada/papers/mihalcea.acl2004.pdf">Graph-based
Ranking Algorithms for Sentence Extraction, Applied to Text
Summarization</A>, in Proceedings of the 42nd Annual Meeting of the
Association for Computational Linguistics, companion volume (ACL
2004), Barcelona, Spain, July 2004;
(c) Paper/data of Pang and Lee on <A
HREF="http://www.cs.cornell.edu/home/llee/papers/cutsent.home.html">sentiment
analysis with min-cuts</A>
<P>
<em>PageRank and variants; HITS; min-cuts</em>
</td>
<td> <A HREF="slides/jimmy_ir_lecture.ppt">IR lecture slides</A>,<BR>
     <A HREF="slides/graph_based_methods.ppt">Graph-methods lecture
     slides</A>.
</td>
<td>
Optional readings of interest:
(a) Christopher D. Manning, Prabhakar Raghavan and Hinrich Schutze,
<A HREF="http://www-csli.stanford.edu/~schuetze/information-retrieval-book.html">Introduction to Information Retrieval, Cambridge University Press</A>:
<A HREF="http://nlp.stanford.edu/IR-book/pdf/chapter21-linkanalysis.pdf">
Chapter 21 "Link Analysis"</A>;
(b) <A HREF="http://dbpubs.stanford.edu:8090/pub/1999-66">Page L. et. al Page Rank Citation Ranking: Bringing Order to the Web</A>;
(c) <A HREF="http://www.cs.cornell.edu/home/kleinber/auth.pdf">Jon Kleinberg  Authoritative sources in a hyperlinked environment, in proceedings of SODA 1998</A>
(d) Kurt Bryan and Tanya Leise, <A  HREF="http://www.rose-hulman.edu/~bryan/google.html">The $25,000,000,000 Eigenvector: The Linear Algebra Behind Google</A> (SIAM Review 48(3), 2006, pp. 569-581)
<td>
-->

<!--
<tr>
<td>Apr 17</td>
<td>Word sense disambiguation<BR>
</td>
<td>Ch 8.5, 15.{1,2,4}<BR>
<em>
 Semantic similarity; relatedness; synonymy; polysemy; homonymy; entailment; ontology-based similarity measures; vector
 representations and similarity measures; sketch of LSA.
 Characterizing the WSD problem; WSD as a  supervised classification problem. Lesk algorithm;
 semi-supervised learning and Yarowsky's algorithm;
 WSD in applications; WSD evaluation.
</em>
</td>
<td></td>
<td>
Optional:  Adam Kilgarriff (1997) <A HREF="http://www.kilgarriff.co.uk/Publications/1997-K-CHum-believe.pdf">I don't believe in word senses</A> Computers and the Humanities 31(2), pp. 91-113;
Philip Resnik (2006), <A HREF="http://www.springerlink.com/content/j227415g22v74686/">WSD in NLP Applications</A> (<A HREF="http://books.google.com/books?id=GLck75U20pAC&lpg=PA299&ots=M3uAfkLHxb&dq=resnik%20wsd%20in%20nlp%20applications&pg=PA299#v=onepage&q=resnik%20wsd%20in%20nlp%20applications&f=false">Google Books</A>)
  </td>
</tr>
-->


<tr>
<td>Apr 23</td>
<td>Machine translation<BR>
</td>
<td>Ch 13 and Adam Lopez, <A HREF="http://homepages.inf.ed.ac.uk/alopez/papers/survey.pdf">Statistical Machine Translation</A>,
In  ACM Computing Surveys  40(3), Article 8, pages 149, August 2008.<BR>
  <P>
  <em><!--COVER Historical view of MT approaches; noisy channel for SMT; IBM
  models 1 and 4; HMM distortion model; going beyond word-level
  models --></em>
</td>
<!-- <A HREF="assignments/parallel_text.html">Assignment</A> or Team
     Project 2 -->
<td>None.</td>
<td>
  Also potentially useful or of interest:
  <UL>
  <LI> Kevin Knight, <A HREF="http://www.isi.edu/natural-language/mt/wkbk.rtf">A Statistical MT Tutorial Workbook</A>;
  <LI> Philipp Koehn, <A
  HREF="http://www.isi.edu/licensed-sw/pharaoh/manual-v1.2.ps">PHARAOH: A
  Beam Search Decoder for Phrase-Based Statistical Machine
  Translation</A>
  <LI> Philipp Koehn (2004) <A HREF="http://www.iccs.informatics.ed.ac.uk/~pkoehn/publications/pharaoh-amta2004-slides.pdf">presentation on PHARAOH decoder</A>
  </UL>
  <!--  <BR>
  <A HREF="http://www.cs.unt.edu/~rada/papers/mihalcea.wpt03.pdf">
  Mihalcea and Pedersen (2003)</A>;
  <BR>
  Philip Resnik, <A HREF="http://umiacs.umd.edu/~resnik/pubs/cicling2004.ps">
  Exploiting Hidden Meanings: Using Bilingual Text for
  Monolingual Annotation</A>. In Alexander Gelbukh (ed.),  Lecture Notes
  in Computer Science 2945: Computational Linguistics and Intelligent
  Text Processing, Springer, 2004, pp. 283-299.
  -->
</td>
</tr>



<tr>
<td>April 30</td>
<td>Machine translation continued<br></td>
<td>
  <!-- <font color="red">This material may be folded into the previous class
  in order to make room for a different topic.</font><p>
  Papineni, Roukos, Ward and Zhu. 2001.
  <A HREF="http://www1.cs.columbia.edu/nlp/sgd/bleu.pdf">BLEU: A Method for Automatic Evaluation
  of Machine Translation</A>
  -->
  <P>
  <em><!--COVER Components of a phrase-based system: language modeling,
  translation modeling; sentence alignment, word
  alignment, phrase extraction, parameter tuning, decoding, rescoring,
  evaluation. --></em>
  </td>
<td>None.</td>
<!--<td> <A HREF="assignments/phrase_based_mt.html">Assignment 6</A><
  or continue Team Project 2   </td> -->
<td>
<!-- Hal DaumÃ© III and Jagadeesh Jagarlamudi,
 <A HREF="http://hal3.name/docs/daume11lexicaladapt.pdf">
 Domain Adaptation for Machine Translation by Mining Unseen Words</A>, ACL 2011.
-->
</td>
</tr>



<!--
<tr>
<td>Apr 30
<td>The Web as a Corpus<BR>
</td>
<td>
  (a) A. Kilgarriff and G. Grefenstette, <A
  HREF="http://citeseer.ist.psu.edu/630648.html">Introduction to the
  special issue on the web as corpus</A>, Computational Linguistics
  29(3): 333-348 (2003) <BR>
  (b) Lapata, Mirella and Frank Keller. 2004. <A
  HREF="http://homepages.inf.ed.ac.uk/mlap/Papers/naacl04a.html">The
  Web as a Baseline: Evaluating the Performance of Unsupervised
  Web-based Models for a Range of NLP Tasks</A>. Proc HLT/NAACL,
  pp. 121-128.
  <BR>
  (c) Lapata, Maria. 2001. <A
  HREF="http://homepages.inf.ed.ac.uk/mlap/Papers/naacl01.html">A
  Corpus-based Account of Regular Polysemy: The Case of
  Context-sensitive Adjectives.</A>, Proc NAACL.
  <BR>
  (d) Philip Resnik, Aaron Elkiss, Ellen Lau and Heather Taylor.  <A
  HREF="http://umiacs.umd.edu/~resnik/pubs/bls2005.pdf">
  The Web in Theoretical Linguistics Research: Two Case Studies Using
  the Linguist's Search Engine.</A>, Proc. 31st Meeting of the Berkeley
  Linguistics Society, pp. 265-276, February 2005.
  <P>
  <em>
  What is a corpus?; using the Web for NLP tasks; ways linguists can use the Web.
  </em>
</td>
<td></td>
<td>
Also of possible interest:
  <A HREF="http://lse.umiacs.umd.edu">Linguist's Search Engine</A>;
  <BR>
  Mirella Lapata, and Frank Keller. 2005. <A
  HREF="http://homepages.inf.ed.ac.uk/mlap/Papers/tslp05.html">Web-based
  Models for Natural Language Processing.<A> ACM Transactions on
  Speech and Language Processing 2:1, 1-31. (Extends Lapata and Keller 2004);
  <BR>
  <A HREF="http://www.webexp.info/">WebExp</A> software for
  Web-based psycholinguistics
</td>
<td>
-->

<!-- <A HREF="http://mitpress.mit.edu/journals/pdf/coli_29_3_333_0.pdf">
Kilgarriff and Grefenstette (2003)</A>;
<A HREF="http://homepages.inf.ed.ac.uk/keller/papers/cl03.pdf">Keller
and Lapata (2003)</A>
  Keller, Frank and Mirella Lapata, <A
HREF="http://homepages.inf.ed.ac.uk/keller/papers/cl03.pdf">Using the
Web to Obtain Frequencies for Unseen Bigrams. Computational
Linguistics 29:3, 459-484, 2003;
Philip Resnik and Aaron Elkiss, <A
HREF="http://umiacs.umd.edu/~resnik/temp/lsedemo_for_class.pdf"> The
Linguist's Search Engine: An Overview [DRAFT]</A>; others?
-->
</td>
</tr>


<tr>
<td>May 7</td>
<td>Projects discussion <BR></td>

<td>
</td>
<td></td>
<td></td>
</tr>



<!--

<tr>
<td> </td>
<td>Lexical acquisition<BR>
</td>
<td>Ch 8 (exc 8.5)</td>
<td>
  <!--
    <A
    HREF="http://umiacs.umd.edu/~resnik/ling773_sp2014/assignment/lexical_acquisition.html">Assignment
    </td>
  -->
<td></td>
</tr>

<!--
<tr>
<td> </td>
<td>Compositional semantics (?)
</td>
<td>
<A
HREF="http://www.stanford.edu/class/cs224n/handouts/cl-semantics-new.pdf">Manning
(2000; rev 2005), An Introduction to Formal Computational Semantics</A>
</td>
<td></td>
<td></td>
</tr>

<tr>
<td> </td>
<td>Computational psycholinguistics (?)<BR>
</td>
<td>
<A HREF="http://www.cs.colorado.edu/~martin/slp.html">
Jurafsky and Martin (2000)</A>, Sections 12.5 (Human Parsing) and 13.4
(Complexity and Human Processing);
<A HREF="http://www.stanford.edu/~jurafsky/prob.pdf">Jurafsky
(2003)</A> (except Sections 2.1, 2.2, 2.5, 2.6, 3.1, 3.5, 3.6).
Connectionist modeling?

(From published book: except Sections 3.2.1, 3.2.2, 3.2.5, 3.2.6, 3.3.1, 3.3.5, 3.3.6)



<tr>
<td> </td>
<td>TBA<BR>
</td>
<td></td>
<td></td>
<td></td>
</tr>

-->

<!--
<tr>
<td><font color="red">FINAL DATE</font></td>
<td><font color="red">TBD</font><BR>
</td>
<td>Officially cumulative but with a strong (at least 80%) emphasis
on material after the midterm.
</td>
<td>Relax!</td>
<td><A HREF="final_guidance.html">Guidance for studying</A> </td>
</tr>
-->



<!--
Jurafsky, Dan. 2003. Probabilistic Modeling in Psycholinguistics:
Linguistic Comprehension and Production. In Rens Bod, Jennifer
Hay, and Stefanie Jannedy, (Eds)., Probabilistic Linguistics.
http://www.stanford.edu/~jurafsky/prob.pdf

Rick Lewis, Computational Psycholinguistics, Encylopedia of Cognitive
Science, Macmillan, 2000.
www-personal.umich.edu/~rickl/Documents/Lewis-CompPsychling.pdf

Crocker, Matthew W. and Frank Keller. 2005. Probabilistic Grammars as
Models of Gradience in Language Processing. To appear in Gisbert
Fanselow, Caroline F	ry, Ralph Vogel, and Matthias Schlesewsky,
eds., Gradience in Grammar: Generative Perspectives. Oxford: Oxford
University Press.
http://homepages.inf.ed.ac.uk/keller/papers/oup05b.pdf

Lewis & Vasishth on interference
http://www.msu.edu/course/lin/875/cogsci-04-jou.pdf

Levy on surprisal and German
http://www.msu.edu/course/lin/875/surprisal-chapter.pdf

Hale syllabus
http://www.msu.edu/course/lin/875/syllabus.html

Morten H. Christiansen,
Connectionist psycholinguistics: The very idea.
In M.H. Christiansen & N. Chater (Eds.), Connectionist
psycholinguistics (pp.1-15). Westport, CT: Ablex.
http://cnl.psych.cornell.edu/papers/CP-intro.pdf

John Nerbonne learning bibliography
http://www.let.rug.nl/~nerbonne/teach/learning/literature.htm
-->


</table>
</center>
</font>
<P>




<!--
<A NAME="labs">
*We may or may not add a few lab sessions, but if we do they will be
held in room 1442 of the <A
HREF="http://www.inform.umd.edu/CampusInfo/Facilities/Buildings/AVW/">
     A.V. Williams building</A>.  Click the link for maps, directions, parking
     information.  To get to Room 1442 come in the main entrance, facing
     the elevators, turn left, and go through the glass doors.  The lab
     will be on your right.
-->
<P>



<A HREF="index.html">Return to course home page</A>
<HR>

<hr>
<P>
</body>
</html>









